# æª”æ¡ˆåç¨±ï¼š2_dashboard.pyï¼ˆç†æ€§ç‰ˆï¼šTop3 æ·±åº¦è§£æ + æ•¸å­—ç·šç´¢äººé¡åŒ–æ‘˜è¦ + Prompt æ³¨å…¥ï¼‰
import os
import re
import json
import hashlib
from urllib.parse import urlparse

import streamlit as st
import pandas as pd
import plotly.express as px
from collections import Counter

# ---- å¯é¸ï¼šrequests / bs4ï¼ˆæ·±åº¦åˆ†æç”¨ï¼‰----
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

st.set_page_config(page_title="å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤", layout="wide")


# =========================
# 0) å°å·¥å…·
# =========================
def safe_str(x, default="ç„¡"):
    if x is None:
        return default
    s = str(x)
    return s if s.strip() else default

def clip_text(s, n=160):
    s = safe_str(s, "")
    return (s[:n] + "â€¦") if len(s) > n else s

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def _dedup_keep_order(items, max_n=10):
    seen = set()
    out = []
    for x in items:
        x = x.strip()
        if not x:
            continue
        if x in seen:
            continue
        out.append(x)
        seen.add(x)
        if len(out) >= max_n:
            break
    return out

def _to_int_safe(s):
    try:
        return int(s)
    except Exception:
        return None


# =========================
# 1) æ·±åº¦åˆ†æï¼šæŠ“é  + è§£æ + å¿«å–ï¼ˆTop3ï¼‰
# =========================
CACHE_DIR = "serp_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
}

FAQ_HINTS = ["å¸¸è¦‹å•é¡Œ", "FAQ", "å•ç­”", "Q&A", "QA", "å•é¡Œ"]

# æŠ“æ•¸å­—ï¼ˆå« %ï¼‰
NUM_PATTERN = r"\d+(?:\.\d+)?%?"
MONEY_PATTERN = r"(\d+(?:\.\d+)?)(\s*è¬|\s*å…ƒ|\s*[kK])"
YEAR_PATTERN = r"(20\d{2}|19\d{2})"
RANGE_PATTERN = r"(\d+(?:\.\d+)?)[\s]*[~ï½\-â€“â€”][\s]*(\d+(?:\.\d+)?)"

KW_SALARY = ["è–ª", "è–ªè³‡", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª", "å¾…é‡", "å…ƒ", "è¬", "k", "K"]
KW_SCORE = ["åˆ†æ•¸", "ç´šåˆ†", "éŒ„å–", "é–€æª»", "æœ€ä½", "çµ±æ¸¬", "ç¹æ˜Ÿ", "ç”„é¸", "è½é»", "PR", "å€ç‡", "ç´šè·"]
KW_CREDITS = ["å­¸åˆ†", "å¿…ä¿®", "é¸ä¿®", "ç¸½å­¸åˆ†", "ç•¢æ¥­å­¸åˆ†"]
KW_PASS = ["åŠæ ¼", "é€šé", "åˆæ ¼", "åŠæ ¼ç‡", "é€šéç‡", "åˆæ ¼ç‡", "éŒ„å–ç‡", "åœ‹è€ƒ", "è­‰ç…§"]

def _cache_key(url: str) -> str:
    return hashlib.md5(url.encode("utf-8")).hexdigest()

def load_cached_page(url: str):
    fp = os.path.join(CACHE_DIR, _cache_key(url) + ".json")
    if os.path.exists(fp):
        try:
            with open(fp, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return None
    return None

def save_cached_page(url: str, data: dict):
    fp = os.path.join(CACHE_DIR, _cache_key(url) + ".json")
    try:
        with open(fp, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def fetch_html(url: str, timeout=10) -> str:
    if not HAS_REQUESTS:
        return ""
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        ct = (r.headers.get("Content-Type") or "").lower()
        if r.status_code >= 400:
            return ""
        if "text/html" not in ct and "application/xhtml" not in ct:
            return ""
        return r.text or ""
    except Exception:
        return ""


# =========================
# 2) æ•¸å­—ç·šç´¢ï¼šåˆ†é¡ + äººé¡åŒ–æ‘˜è¦
# =========================
def classify_number_clues(text: str) -> dict:
    """
    å…ˆæŠŠé é¢è£¡çš„æ•¸å­—æŠ“å‡ºä¾†ï¼Œä¾ä¸Šä¸‹æ–‡åˆ†é¡ï¼š
    salary / score / credits / passrate
    """
    clues = {"salary": [], "score": [], "credits": [], "passrate": []}
    if not text:
        return clues

    t = text.replace("ï¼…", "%")

    for m in re.finditer(NUM_PATTERN, t):
        val = m.group(0)
        s = max(0, m.start() - 26)
        e = min(len(t), m.end() + 26)
        ctx = t[s:e].strip()
        if len(ctx) > 90:
            ctx = ctx[:90] + "â€¦"

        # åŠæ ¼ç‡/é€šéç‡ï¼ˆå« % ä¸”é™„è¿‘æœ‰é€šé/åŠæ ¼/è­‰ç…§/åœ‹è€ƒï¼‰
        if ("%" in val or "%" in ctx) and any(k in ctx for k in KW_PASS):
            clues["passrate"].append(ctx)
            continue

        # å­¸åˆ†ï¼ˆçœ‹åˆ°å­¸åˆ†/å¿…ä¿®/é¸ä¿®ï¼‰
        if any(k in ctx for k in KW_CREDITS) or ("å­¸åˆ†" in ctx):
            clues["credits"].append(ctx)
            continue

        # è–ªè³‡ï¼ˆè–ª/å…ƒ/è¬/kï¼‰
        if any(k in ctx for k in KW_SALARY):
            clues["salary"].append(ctx)
            continue

        # åˆ†æ•¸/é–€æª»ï¼ˆå¿…é ˆæœ‰é—œéµè©æ‰ç®—ï¼‰
        if any(k in ctx for k in KW_SCORE):
            clues["score"].append(ctx)
            continue

    for k in clues:
        clues[k] = _dedup_keep_order(clues[k], max_n=12)
    return clues


def _extract_money_values(ctx: str):
    """
    å¾ä¸Šä¸‹æ–‡æŠ“å¯èƒ½çš„è–ªè³‡æ•¸å€¼ï¼Œå›å‚³ list[(value, unit)]ï¼Œunit: 'å…ƒ'/'è¬'/'k'
    """
    vals = []
    for m in re.finditer(MONEY_PATTERN, ctx):
        num = m.group(1)
        unit = m.group(2).strip()
        vals.append((num, unit))
    return vals


def _normalize_money(num_str, unit):
    """
    å˜—è©¦æŠŠè–ªè³‡è½‰ç‚ºã€Œæœˆè–ªå…ƒã€ä¼°è¨ˆï¼ˆå¾ˆç²—ç•¥ä½†å¯ç”¨ï¼‰
    - 'è¬' => *10000
    - 'k' => *1000
    - 'å…ƒ' => åŸæ¨£
    å›å‚³ int æˆ– None
    """
    try:
        x = float(num_str)
    except Exception:
        return None

    u = unit.lower()
    if "è¬" in u:
        return int(x * 10000)
    if "k" in u:
        return int(x * 1000)
    if "å…ƒ" in u:
        return int(x)
    return None


def summarize_salary(clues_salary: list) -> dict:
    """
    å°‡è–ªè³‡ç·šç´¢æ•´ç†æˆæ›´åƒäººé¡çš„è¼¸å‡ºï¼š
    - å„ªå…ˆæ‰¾å€é–“ï¼ˆä¾‹å¦‚ 3.2~3.8 è¬ã€32000~38000ï¼‰
    - å†æ‰¾å–®å€¼ï¼ˆä¾‹å¦‚ 35kï¼‰
    - åˆ¤æ–·æœˆè–ª/å¹´è–ª/èµ·è–ªå­—æ¨£
    """
    if not clues_salary:
        return {"found": False, "type": "ç„¡", "range": None, "points": [], "note": ""}

    types = {"æœˆè–ª": 0, "å¹´è–ª": 0, "èµ·è–ª": 0}
    money_nums = []
    ranges = []

    points = []
    for ctx in clues_salary[:12]:
        points.append(ctx)
        for t in types:
            if t in ctx:
                types[t] += 1

        # æŠ“å€é–“ï¼ˆæ•¸å­—~æ•¸å­—ï¼‰
        rm = re.search(RANGE_PATTERN, ctx)
        if rm and any(k in ctx for k in ["è¬", "å…ƒ", "k", "K", "è–ª", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª"]):
            a, b = rm.group(1), rm.group(2)
            # å˜—è©¦å¾ ctx æ¨ unit
            unit = "å…ƒ" if "å…ƒ" in ctx else ("è¬" if "è¬" in ctx else ("k" if ("k" in ctx or "K" in ctx) else "å…ƒ"))
            va = _normalize_money(a, unit)
            vb = _normalize_money(b, unit)
            if va and vb:
                lo, hi = min(va, vb), max(va, vb)
                ranges.append((lo, hi, ctx))

        # æŠ“å–®å€¼
        for num, unit in _extract_money_values(ctx):
            v = _normalize_money(num, unit)
            if v:
                money_nums.append((v, ctx))

    # åˆ¤æ–·æœ€å¯èƒ½çš„é¡å‹
    best_type = max(types, key=lambda k: types[k])
    if types[best_type] == 0:
        best_type = "è–ªè³‡"

    # çµ„åˆæ‘˜è¦
    summary_range = None
    if ranges:
        # å–æœ€åˆç†çš„ç¯„åœï¼ˆå»æ‰æ¥µç«¯ï¼š>200k æœˆè–ªçš„å…ˆä¸ä¿¡ï¼‰
        sane = [r for r in ranges if 15000 <= r[0] <= 200000 and 15000 <= r[1] <= 200000]
        use = sane[0] if sane else ranges[0]
        summary_range = (use[0], use[1])

    note = "å»ºè­°ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€æè¿°ï¼Œé¿å…åªå¯«å–®ä¸€æ­»æ•¸å­—ã€‚"
    return {
        "found": True,
        "type": best_type,
        "range": summary_range,
        "points": _dedup_keep_order(points, max_n=6),
        "note": note
    }


def summarize_score(clues_score: list) -> dict:
    """
    åˆ†æ•¸/é–€æª»ï¼šä¸ç¡¬ç®—ç²¾æº–æ•¸å­—ï¼Œåç†æ€§å¯«æ³•ï¼šã€è¿‘2-3å¹´å€é–“ã€ã€ä»¥å®˜æ–¹ç°¡ç« ç‚ºæº–ã€
    """
    if not clues_score:
        return {"found": False, "points": [], "note": ""}

    points = _dedup_keep_order(clues_score, max_n=6)
    note = "é–€æª»é€šå¸¸æœƒéš¨å¹´åº¦æµ®å‹•ï¼Œå»ºè­°ç”¨ã€è¿‘ 2â€“3 å¹´å€é–“ã€å‘ˆç¾ï¼Œä¸¦å¼•ç”¨å®˜æ–¹æ‹›ç”Ÿç°¡ç« /åˆ†ç™¼è³‡æ–™ã€‚"
    return {"found": True, "points": points, "note": note}


def summarize_credits(clues_credits: list) -> dict:
    """
    å­¸åˆ†ï¼šå˜—è©¦æŠ“ã€ç¸½å­¸åˆ†/å¿…ä¿®/é¸ä¿®ã€ï¼ŒæŠ“ä¸åˆ°å°±è¼¸å‡ºç†æ€§æ¨¡æ¿ã€‚
    """
    if not clues_credits:
        return {"found": False, "total": None, "required": None, "elective": None, "points": [], "note": ""}

    text = " ".join(clues_credits[:10])
    # ç²—æŠ“ï¼šä¾‹å¦‚ã€Œç•¢æ¥­ç¸½å­¸åˆ† 128ã€ã€Œå¿…ä¿® 90ã€ã€Œé¸ä¿® 38ã€
    total = None
    required = None
    elective = None

    m_total = re.search(r"(ç¸½å­¸åˆ†|ç•¢æ¥­å­¸åˆ†)[^\d]{0,6}(\d{2,3})", text)
    if m_total:
        total = _to_int_safe(m_total.group(2))

    m_req = re.search(r"(å¿…ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_req:
        required = _to_int_safe(m_req.group(2))

    m_ele = re.search(r"(é¸ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_ele:
        elective = _to_int_safe(m_ele.group(2))

    points = _dedup_keep_order(clues_credits, max_n=6)
    note = "å­¸åˆ†/èª²ç¨‹ä»¥ç³»ç¶²èª²ç¨‹åœ°åœ–æˆ–èª²ç¨‹æŸ¥è©¢ç³»çµ±ç‚ºæº–ï¼›ç”¨è¡¨æ ¼å‘ˆç¾æœ€æ¸…æ¥šã€‚"
    return {"found": True, "total": total, "required": required, "elective": elective, "points": points, "note": note}


def summarize_passrate(clues_pass: list) -> dict:
    """
    åŠæ ¼ç‡/é€šéç‡ï¼šæŠ“åˆ° % å°±æ•´ç†æˆã€å¯èƒ½çš„é€šéç‡æ•˜è¿°ã€ï¼Œä½†ä»æé†’ä»¥å®˜æ–¹/å¯æŸ¥ä¾†æºç‚ºæº–ã€‚
    """
    if not clues_pass:
        return {"found": False, "rates": [], "points": [], "note": ""}

    points = _dedup_keep_order(clues_pass, max_n=6)
    rates = []
    for ctx in points:
        for p in re.findall(r"\d+(?:\.\d+)?%", ctx):
            rates.append(p)
    rates = _dedup_keep_order(rates, max_n=6)

    note = "é€šéç‡/åŠæ ¼ç‡å‹™å¿…æ¨™ç¤ºå¹´ä»½èˆ‡ä¾†æºï¼ˆè€ƒé¸éƒ¨/å®˜æ–¹å…¬å‘Š/æ ¡æ–¹å…¬é–‹æˆæœï¼‰ï¼Œé¿å…è¢«è³ªç–‘ã€‚"
    return {"found": True, "rates": rates, "points": points, "note": note}


def humanize_number_output(agg_clues: dict) -> dict:
    """
    æŠŠèšåˆçš„åˆ†é¡ç·šç´¢ â†’ ç”¢å‡ºæ›´åƒäººé¡å¯«çš„ã€ç†æ€§æ‘˜è¦ã€çµæ§‹ã€‚
    """
    out = {
        "salary": summarize_salary(agg_clues.get("salary", [])),
        "score": summarize_score(agg_clues.get("score", [])),
        "credits": summarize_credits(agg_clues.get("credits", [])),
        "passrate": summarize_passrate(agg_clues.get("passrate", []))
    }
    return out


def build_rational_citation_paragraphs(human: dict) -> str:
    """
    å°‡äººé¡åŒ–æ‘˜è¦ â†’ ç”Ÿæˆå¯ç›´æ¥è²¼æ–‡ç« çš„ã€Œç†æ€§å¼•ç”¨æ®µè½ã€ï¼ˆåç†æ€§ï¼Œä¸ç…½æƒ…ï¼‰ã€‚
    """
    paras = []

    # è–ªè³‡
    sal = human.get("salary", {})
    if sal.get("found"):
        r = sal.get("range")
        if r:
            lo, hi = r
            # è½‰æˆã€Œè¬ã€é¡¯ç¤ºæ¯”è¼ƒäººé¡
            lo_w = round(lo / 10000, 1)
            hi_w = round(hi / 10000, 1)
            line = f"ä»¥ç›®å‰ç¶²è·¯å¯è¦‹è³‡æ–™ä¾†çœ‹ï¼Œè–ªè³‡å¤šä»¥ã€å€é–“ã€å‘ˆç¾ï¼Œç´„è½åœ¨ **{lo_w}ï½{hi_w} è¬/æœˆ**ï¼ˆæœƒä¾åœ°å€ã€ç­åˆ¥ã€è·å‹™è€Œè®Šå‹•ï¼‰ã€‚"
        else:
            line = "è–ªè³‡è³‡è¨Šå¤šåŠå»ºè­°ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€æè¿°ï¼Œé¿å…å–®ä¸€æ•¸å­—é€ æˆèª¤å°ã€‚"

        paras.append(
            "### è–ªè³‡ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            f"{line}\n"
            "- **å¯«æ³•å»ºè­°**ï¼šç”¨ã€èµ·è–ª/1â€“3 å¹´/3â€“5 å¹´ã€åˆ†æ®µï¼Œæˆ–ç”¨ã€è·å‹™åˆ¥ã€åˆ†æ®µã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼š104/äººåŠ›éŠ€è¡Œè·ç¼ºè–ªè³‡å€é–“ã€é†«é™¢æ‹›å‹Ÿå…¬å‘Šï¼ˆæ¨™ç¤ºå¹´ä»½/ä¾†æºï¼‰ã€‚"
        )

    # åˆ†æ•¸/é–€æª»
    sc = human.get("score", {})
    if sc.get("found"):
        paras.append(
            "### åˆ†æ•¸/é–€æª»ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            "éŒ„å–é–€æª»é€šå¸¸æœƒéš¨å¹´åº¦æµ®å‹•ï¼Œå› æ­¤æ¯”è¼ƒç©©çš„å‘ˆç¾æ–¹å¼æ˜¯ï¼š**æ•´ç†è¿‘ 2â€“3 å¹´å€é–“**ï¼Œä¸¦æ¸…æ¥šæ¨™è¨»ã€å…¥å­¸ç®¡é“ã€ï¼ˆçµ±æ¸¬åˆ†ç™¼/ç”„é¸/ç¹æ˜Ÿç­‰ï¼‰ã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šå„æ ¡æ‹›ç”Ÿç°¡ç« ã€ç”„é¸å…¥å­¸ç°¡ç« ã€çµ±æ¸¬åˆ†ç™¼è³‡æ–™ï¼ˆä»¥å®˜æ–¹ç‰ˆæœ¬ç‚ºæº–ï¼‰ã€‚"
        )

    # å­¸åˆ†/èª²ç¨‹
    cr = human.get("credits", {})
    if cr.get("found"):
        t = cr.get("total")
        req = cr.get("required")
        ele = cr.get("elective")
        if t or req or ele:
            rows = []
            if t: rows.append(f"- ç•¢æ¥­ç¸½å­¸åˆ†ï¼š{t}")
            if req: rows.append(f"- å¿…ä¿®ï¼š{req}")
            if ele: rows.append(f"- é¸ä¿®ï¼š{ele}")
            detail = "\n".join(rows)
        else:
            detail = "- å»ºè­°ç›´æ¥æ”¾ã€èª²ç¨‹åœ°åœ–/å­¸åˆ†çµæ§‹è¡¨ã€ï¼ˆè®€è€…æœƒä¸€çœ¼æ‡‚ï¼‰ã€‚"

        paras.append(
            "### å­¸åˆ†/èª²ç¨‹ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            "èª²ç¨‹è³‡è¨Šæœ€æœ‰æ•ˆçš„å‘ˆç¾æ–¹å¼ï¼Œæ˜¯æŠŠã€å­¸åˆ†çµæ§‹ã€ç”¨è¡¨æ ¼èªªæ¸…æ¥šï¼Œä¸¦æ­é…ã€å¹´ç´šå­¸ç¿’è·¯å¾‘ã€ï¼ˆå¤§ä¸€æ‰“åº•â†’å¤§äºŒå°ˆæ¥­â†’å¾ŒçºŒå¯¦ç¿’/å°ˆé¡Œï¼‰ã€‚\n"
            f"{detail}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šç³»ç¶²èª²ç¨‹è¦åŠƒã€èª²ç¨‹æŸ¥è©¢ç³»çµ±ã€æ‹›ç”Ÿç°¡ç« é™„éŒ„ã€‚"
        )

    # åŠæ ¼ç‡/é€šéç‡
    pr = human.get("passrate", {})
    if pr.get("found"):
        rates = pr.get("rates") or []
        if rates:
            rate_line = "ç¶²è·¯é é¢ä¸­å¯è¦‹çš„é€šéç‡/åŠæ ¼ç‡ç‰‡æ®µåŒ…å«ï¼š" + "ã€".join(rates) + "ï¼ˆä»éœ€ä»¥å®˜æ–¹/å¯æŸ¥ä¾†æºæ ¸å°å¹´ä»½èˆ‡å£å¾‘ï¼‰ã€‚"
        else:
            rate_line = "è‹¥æ–‡ç« è¦æåˆ°é€šéç‡/åŠæ ¼ç‡ï¼Œå‹™å¿…æ¨™ç¤ºå¹´ä»½èˆ‡ä¾†æºï¼Œé¿å…è¢«è³ªç–‘ã€‚"

        paras.append(
            "### åœ‹è€ƒ/è­‰ç…§é€šéç‡ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            f"{rate_line}\n"
            "- **å¯«æ³•å»ºè­°**ï¼šä¸è¦åªä¸Ÿä¸€å€‹ %ï¼Œè¦äº¤ä»£ã€å¹´ä»½ã€æ¯æ•¸ã€è€ƒè©¦ç¨®é¡ã€ã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šè€ƒé¸éƒ¨/å®˜æ–¹å…¬å‘Šã€å…¬æœƒè³‡è¨Šã€æ ¡æ–¹å…¬é–‹æˆæœï¼ˆé™„å¹´ä»½ï¼‰ã€‚"
        )

    if not paras:
        return (
            "### å»ºè­°å¼•ç”¨æ®µè½ï¼ˆé€šç”¨ç†æ€§ç‰ˆï¼‰\n"
            "è‹¥ç›®å‰ Top3 çš„å…§å®¹ç¼ºå°‘å¯æŸ¥è­‰æ•¸æ“šï¼Œå»ºè­°ç”¨ã€å®˜æ–¹ä¾†æº + è¡¨æ ¼æ•´ç† + FAQã€è£œé½Šã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šç³»ç¶²ã€æ‹›ç”Ÿç°¡ç« ã€æ”¿åºœ/å…¬æœƒè³‡è¨Šã€104ã€‚"
        )

    return "\n\n".join(paras)


# =========================
# 3) è§£æç¶²é 
# =========================
def parse_competitor_page(url: str) -> dict:
    """
    è§£æå°æ‰‹é é¢ï¼šæŠ“ H1/H2/H3ã€metaã€è¡¨æ ¼/FAQ/æ¢åˆ—ï¼Œ
    ä¸¦æŠŠæ•¸å­—ç·šç´¢åˆ†é¡ï¼ˆè–ªè³‡/åˆ†æ•¸/å­¸åˆ†/åŠæ ¼ç‡ï¼‰ã€‚
    """
    cached = load_cached_page(url)
    if cached:
        return cached

    html = fetch_html(url)
    if not html:
        data = {"url": url, "ok": 0, "reason": "fetch_failed"}
        save_cached_page(url, data)
        return data

    # --- æ²’ bs4ï¼šé€€åŒ– regex ---
    if not HAS_BS4:
        lower = html.lower()
        text = re.sub(r"<script[\s\S]*?</script>", " ", html, flags=re.I)
        text = re.sub(r"<style[\s\S]*?</style>", " ", text, flags=re.I)
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        has_faq = 1 if any(h.lower() in text.lower() for h in FAQ_HINTS) else 0
        number_clues = classify_number_clues(text)

        data = {
            "url": url, "ok": 1,
            "title": "", "meta_desc": "",
            "h1": "", "h2": [], "h3": [],
            "has_table": 1 if "<table" in lower else 0,
            "has_list": 1 if ("<ul" in lower or "<ol" in lower) else 0,
            "has_faq": has_faq,
            "number_clues": number_clues,
            "text_preview": text[:900],
            "bullets": [],
        }
        save_cached_page(url, data)
        return data

    # --- bs4ï¼šè¼ƒæº– ---
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    title = soup.title.get_text(strip=True) if soup.title else ""
    meta = soup.find("meta", attrs={"name": "description"})
    meta_desc = meta.get("content", "").strip() if meta else ""

    h1_tag = soup.find("h1")
    h1 = h1_tag.get_text(" ", strip=True) if h1_tag else ""

    h2 = [x.get_text(" ", strip=True) for x in soup.find_all("h2")][:25]
    h3 = [x.get_text(" ", strip=True) for x in soup.find_all("h3")][:25]

    has_table = 1 if soup.find("table") else 0
    has_list = 1 if soup.find(["ul", "ol"]) else 0

    text = soup.get_text(" ", strip=True)
    has_faq = 1 if any(h in text for h in FAQ_HINTS) else 0

    bullets = []
    for ul in soup.find_all(["ul", "ol"])[:3]:
        lis = ul.find_all("li")
        for li in lis[:8]:
            t = li.get_text(" ", strip=True)
            if 8 <= len(t) <= 90:
                bullets.append(t)
    bullets = _dedup_keep_order(bullets, max_n=14)

    number_clues = classify_number_clues(text)

    data = {
        "url": url, "ok": 1,
        "title": title,
        "meta_desc": meta_desc,
        "h1": h1,
        "h2": h2,
        "h3": h3,
        "has_table": has_table,
        "has_list": has_list,
        "has_faq": has_faq,
        "number_clues": number_clues,
        "bullets": bullets,
        "text_preview": text[:900],
    }
    save_cached_page(url, data)
    return data


# =========================
# 4) è®€å–æ•¸æ“š + é˜²å‘†æ¸…ç†
# =========================
try:
    df = pd.read_csv("school_data.csv")
except FileNotFoundError:
    st.error("âŒ æ‰¾ä¸åˆ° school_data.csvï¼Œè«‹å…ˆåŸ·è¡Œ powergeo.py ç”¢å‡ºæ•¸æ“šã€‚")
    st.stop()

TEXT_COLS = [
    "College","Department","Keyword","Keyword_Type","Strategy_Tag",
    "Rank1_Title","Rank1_Link","Rank1_Snippet",
    "Rank2_Title","Rank2_Link","Rank2_Snippet",
    "Rank3_Title","Rank3_Link","Rank3_Snippet",
    "Competitor_Hit"
]
NUM_COLS = [
    "Search_Volume", "Trends_Score", "Trends_Fetched",
    "Opportunity_Score","AI_Potential",
    "Authority_Count","Forum_Count","Answerable_Avg",
    "Citable_Score","Fetch_OK_Count",
    "Schema_Hit_Count",
    "Has_FAQ","Has_Table","Has_List","Has_Headings",
    "Page_Word_Count_Max",
    "Result_Count"
]

for c in TEXT_COLS:
    if c not in df.columns:
        df[c] = "ç„¡"
for c in NUM_COLS:
    if c not in df.columns:
        df[c] = 0

df[TEXT_COLS] = df[TEXT_COLS].fillna("ç„¡").astype(str)
for c in NUM_COLS:
    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)


# =========================
# 5) å´é‚Šæ¬„ï¼šç¯©é¸å™¨
# =========================
st.sidebar.title("ğŸ« å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤")

college_list = ["å…¨éƒ¨å­¸é™¢"] + sorted(df["College"].unique().tolist())
selected_college = st.sidebar.selectbox("STEP 1: é¸æ“‡å­¸é™¢", college_list)

if selected_college == "å…¨éƒ¨å­¸é™¢":
    dept_options = ["å…¨æ ¡ç¸½è¦½"] + sorted(df["Department"].unique().tolist())
else:
    dept_options = ["å­¸é™¢ç¸½è¦½"] + sorted(df[df["College"] == selected_college]["Department"].unique().tolist())

selected_dept = st.sidebar.selectbox("STEP 2: é¸æ“‡ç§‘ç³»/è¦–è§’", dept_options)

kw_types = ["å…¨éƒ¨æ„åœ–"] + sorted(df["Keyword_Type"].unique().tolist())
selected_kw_type = st.sidebar.selectbox("STEP 3: ç¯©é¸æœå°‹æ„åœ–", kw_types)

min_ai = st.sidebar.slider("AI_Potential æœ€ä½é–€æª»", 0, 100, 0, 5)
min_opp_max = int(max(1, df["Opportunity_Score"].max()))
min_opp = st.sidebar.slider("Opportunity_Score æœ€ä½é–€æª»", 0, min_opp_max, 0, 10)

def volume_col(scope_df: pd.DataFrame) -> str:
    if "Trends_Score" in scope_df.columns and scope_df["Trends_Score"].sum() > 0:
        return "Trends_Score"
    return "Search_Volume"

target_df = df.copy()
if selected_college != "å…¨éƒ¨å­¸é™¢":
    target_df = target_df[target_df["College"] == selected_college]
if selected_kw_type != "å…¨éƒ¨æ„åœ–":
    target_df = target_df[target_df["Keyword_Type"] == selected_kw_type]
target_df = target_df[target_df["AI_Potential"] >= min_ai]
target_df = target_df[target_df["Opportunity_Score"] >= min_opp]


# =========================
# 6) ç¸½è¦½é 
# =========================
def overview_page(scope_df: pd.DataFrame, title_prefix: str):
    st.title(f"ğŸ“Š {title_prefix}ï¼šGEO/AI æˆ°ç•¥åœ°åœ–")

    vcol = volume_col(scope_df)

    c1, c2, c3, c4, c5 = st.columns(5)
    with c1:
        st.metric("é—œéµå­—ç­†æ•¸", int(len(scope_df)))
    with c2:
        st.metric("å¹³å‡ Opportunity", round(scope_df["Opportunity_Score"].mean(), 1) if len(scope_df) else 0)
    with c3:
        st.metric("å¹³å‡ AI_Potential", round(scope_df["AI_Potential"].mean(), 1) if len(scope_df) else 0)
    with c4:
        st.metric("å¹³å‡ Citable", round(scope_df["Citable_Score"].mean(), 1) if len(scope_df) else 0)
    with c5:
        label = "å¹³å‡ Trends è²é‡" if vcol == "Trends_Score" else "å¹³å‡è²é‡"
        st.metric(label, round(scope_df[vcol].mean(), 2) if len(scope_df) else 0)

    st.divider()

    colA, colB = st.columns([2, 1])
    with colA:
        dept_rank = (
            scope_df.groupby("Department", as_index=False)["Opportunity_Score"]
            .mean()
            .sort_values("Opportunity_Score", ascending=False)
        )
        fig = px.bar(dept_rank, x="Department", y="Opportunity_Score", color="Department",
                     title="å„ç³» GEO æ©Ÿæœƒå€¼æ’è¡Œï¼ˆå¹³å‡ï¼‰")
        st.plotly_chart(fig, use_container_width=True)

    with colB:
        fig2 = px.pie(scope_df, names="Keyword_Type", title="æœå°‹æ„åœ–åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)

    st.divider()

    colC, colD = st.columns(2)
    with colC:
        vol_rank = (
            scope_df.groupby("Department", as_index=False)[vcol]
            .mean()
            .sort_values(vcol, ascending=False)
        )
        title = "å„ç³» Google Trends ç›¸å°è²é‡ï¼ˆå¹³å‡ï¼‰" if vcol == "Trends_Score" else "å„ç³»è²é‡æŒ‡æ¨™ï¼ˆå¹³å‡ï¼‰"
        fig3 = px.bar(vol_rank, x="Department", y=vcol, color="Department", title=title)
        st.plotly_chart(fig3, use_container_width=True)

    with colD:
        fig4 = px.scatter(
            scope_df, x="Authority_Count", y="Citable_Score",
            size="Opportunity_Score",
            hover_data=["Department", "Keyword", "Rank1_Title"],
            title="å¯å¼•ç”¨æ€§ï¼ˆCitableï¼‰ vs æ¬Šå¨ä¾†æºæ•¸ï¼ˆAuthorityï¼‰"
        )
        st.plotly_chart(fig4, use_container_width=True)

    st.divider()
    st.subheader("ğŸ“‹ ç†±é»é—œéµå­—ç¸½è¡¨")

    show_cols = [
        "College","Department","Keyword","Keyword_Type",
        "Opportunity_Score","AI_Potential",
        "Citable_Score","Authority_Count","Forum_Count",
        vcol,"Trends_Fetched","Rank1_Title"
    ]
    show_cols = [c for c in show_cols if c in scope_df.columns]

    st.dataframe(
        scope_df[show_cols].sort_values(["Opportunity_Score","AI_Potential"], ascending=False),
        use_container_width=True,
        height=560
    )


# =========================
# 7) å–®ä¸€ç§‘ç³»é ï¼ˆæ·±åº¦åˆ†æ + ç†æ€§ Promptï¼‰
# =========================
def dept_page(scope_df: pd.DataFrame, dept_name: str):
    st.title(f"ğŸ” {dept_name}ï¼šç«¶å“ + GEO/AI + ç†æ€§æ·±åº¦åˆ†ææ–‡æ¡ˆç”Ÿæˆå™¨")

    dept_df = scope_df[scope_df["Department"] == dept_name].copy()
    if dept_df.empty:
        st.warning("é€™å€‹ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰è³‡æ–™ã€‚å¯ä»¥æŠŠå·¦é‚Šé–€æª»èª¿ä½ä¸€é»å†çœ‹ã€‚")
        st.stop()

    dept_df = dept_df.sort_values(["Opportunity_Score","AI_Potential"], ascending=False)
    vcol = volume_col(dept_df)

    k1, k2, k3, k4, k5 = st.columns(5)
    with k1: st.metric("é—œéµå­—ç­†æ•¸", int(len(dept_df)))
    with k2: st.metric("å¹³å‡ Opportunity", round(dept_df["Opportunity_Score"].mean(), 1))
    with k3: st.metric("å¹³å‡ AI_Potential", round(dept_df["AI_Potential"].mean(), 1))
    with k4: st.metric("å¹³å‡ Citable", round(dept_df["Citable_Score"].mean(), 1))
    with k5:
        label = "å¹³å‡ Trends è²é‡" if vcol == "Trends_Score" else "å¹³å‡è²é‡"
        st.metric(label, round(dept_df[vcol].mean(), 2))

    st.divider()

    colX, colY = st.columns([2, 1])
    with colX:
        fig = px.box(dept_df, x="Keyword_Type", y="Opportunity_Score", title="ä¸åŒæ„åœ–çš„æ©Ÿæœƒå€¼åˆ†ä½ˆ")
        st.plotly_chart(fig, use_container_width=True)
    with colY:
        fig2 = px.bar(
            dept_df.groupby("Keyword_Type", as_index=False)["AI_Potential"].mean().sort_values("AI_Potential", ascending=False),
            x="Keyword_Type", y="AI_Potential", title="å„æ„åœ–å¹³å‡ AI_Potential"
        )
        st.plotly_chart(fig2, use_container_width=True)

    st.divider()
    st.subheader("ğŸ•µï¸ é¸ä¸€å€‹é—œéµå­—ï¼Œçœ‹ Top3 + æ·±åº¦è§£æ")

    dept_df["Display_Label"] = dept_df["Keyword"] + " [" + dept_df["Keyword_Type"] + "]"
    target_label = st.selectbox("è«‹é¸æ“‡é—œéµå­—", dept_df["Display_Label"].unique())
    target_row = dept_df[dept_df["Display_Label"] == target_label].iloc[0]

    kw = safe_str(target_row["Keyword"])
    strategy = safe_str(target_row["Strategy_Tag"])
    kw_type = safe_str(target_row["Keyword_Type"])

    st.markdown("#### ğŸ§  æ·±åº¦åˆ†æï¼ˆæŠ“ Top3 ç¶²é ï¼šH2/è¡¨æ ¼/FAQ/æ•¸å­— â†’ è½‰æˆç†æ€§æ‘˜è¦ï¼‰")
    if not HAS_REQUESTS:
        st.warning("ä½ çš„ç’°å¢ƒç¼ºå°‘ requestsï¼Œç„¡æ³•æ·±åº¦åˆ†æã€‚è«‹ pip install requests")
        deep_on = False
        run_deep = False
    else:
        deep_on = st.checkbox("å•Ÿç”¨æ·±åº¦åˆ†æï¼ˆç¬¬ä¸€æ¬¡æœƒæ…¢ä¸€é»ï¼›æœ‰å¿«å–ï¼‰", value=False)
        run_deep = st.button("é–‹å§‹æŠ“å–ä¸¦åˆ†æ Top3")

    st.divider()

    col_l, col_r = st.columns([1, 2])

    with col_l:
        st.metric("Opportunity", round(float(target_row["Opportunity_Score"]), 1))
        st.metric("AI_Potential", int(target_row["AI_Potential"]))
        st.metric("Citable", round(float(target_row["Citable_Score"]), 1))

        label = "Trends è²é‡" if vcol == "Trends_Score" else "è²é‡"
        st.metric(label, round(float(target_row.get(vcol, 0)), 2))

        st.metric("Authority", int(target_row["Authority_Count"]))
        st.metric("Forum", int(target_row["Forum_Count"]))

        st.caption("çµæ§‹åŒ–ç‰¹å¾µï¼ˆè¶Šå¤šè¶Šå®¹æ˜“è¢« AI æ‘˜éŒ„ï¼‰")
        s_cols = st.columns(4)
        s_cols[0].metric("FAQ", int(target_row["Has_FAQ"]))
        s_cols[1].metric("Table", int(target_row["Has_Table"]))
        s_cols[2].metric("List", int(target_row["Has_List"]))
        s_cols[3].metric("H2/H3", int(target_row["Has_Headings"]))

        st.info(f"ç­–ç•¥å»ºè­°ï¼š{strategy}")

    competitor_info_text = ""
    deep_briefs = []
    gap_pool_h2 = []
    agg_number_clues = {"salary": [], "score": [], "credits": [], "passrate": []}

    with col_r:
        st.markdown(f"### ğŸ‘€ ã€Œ{kw}ã€Top 3 çµæœ")

        for i in range(1, 4):
            title = safe_str(target_row.get(f"Rank{i}_Title", "ç„¡"))
            link = safe_str(target_row.get(f"Rank{i}_Link", "#"))
            snippet = safe_str(target_row.get(f"Rank{i}_Snippet", ""))

            if title == "ç„¡":
                continue

            competitor_info_text += f"{i}. æ¨™é¡Œï¼š{title}\n   æ‘˜è¦ï¼š{clip_text(snippet, 140)}\n"

            with st.container(border=True):
                st.markdown(f"**#{i} [{title}]({link})**")
                if snippet.strip():
                    st.caption(clip_text(snippet, 260))

            if deep_on and run_deep and link not in ["#", "ç„¡", ""]:
                info = parse_competitor_page(link)
                if info.get("ok") == 1:
                    deep_briefs.append((i, info))
                    gap_pool_h2.extend(info.get("h2", [])[:15])

                    nc = info.get("number_clues", {}) or {}
                    for k in agg_number_clues.keys():
                        agg_number_clues[k].extend(nc.get(k, []))

    # Content Gapï¼ˆH2ï¼‰
    gap_suggestions = []
    if deep_briefs:
        top1_h2 = set(deep_briefs[0][1].get("h2", []))
        freq = Counter([h for h in gap_pool_h2 if 4 <= len(h) <= 24])
        for h, _ in freq.most_common(12):
            if h not in top1_h2:
                gap_suggestions.append(h)
        gap_suggestions = gap_suggestions[:8]

    for k in agg_number_clues:
        agg_number_clues[k] = _dedup_keep_order(agg_number_clues[k], max_n=12)

    human = humanize_number_output(agg_number_clues)
    rational_paras = build_rational_citation_paragraphs(human)

    if deep_on and run_deep:
        st.divider()
        st.subheader("ğŸ“Œ æ·±åº¦æ‘˜è¦ï¼ˆç†æ€§ç‰ˆï¼šæŠŠæ•¸å­—ç·šç´¢æ•´ç†æˆå¯ç”¨çµè«–ï¼‰")

        # ç†æ€§æ‘˜è¦å¡
        with st.container(border=True):
            st.markdown("#### â‘  æ•¸å­—ç·šç´¢ â†’ ç†æ€§çµè«–ï¼ˆå¯ç›´æ¥æ”¾é€²æ–‡ç« ï¼‰")
            st.markdown(rational_paras)

        # ç«¶å“é é¢æ‘˜è¦ï¼ˆå¯é¸çœ‹ï¼‰
        st.subheader("â‘¡ ç«¶å“é é¢çµæ§‹æ‘˜è¦ï¼ˆçµ¦ä½ å°ç…§ç”¨ï¼‰")
        for idx, info in deep_briefs:
            with st.expander(f"#{idx} {domain_of(info['url'])}ï½œ{info.get('title','')[:50]}"):
                st.write(f"**H1ï¼š** {safe_str(info.get('h1','ç„¡'))}")
                if info.get("meta_desc"):
                    st.write(f"**Metaï¼š** {info.get('meta_desc')}")
                st.write(f"**çµæ§‹åŒ–ï¼š** FAQ={info.get('has_faq',0)}ï½œTable={info.get('has_table',0)}ï½œList={info.get('has_list',0)}")
                if info.get("h2"):
                    st.write("**H2ï¼š** " + " / ".join(info["h2"][:12]))
                if info.get("bullets"):
                    st.write("**æ¢åˆ—ï¼š**")
                    for b in info["bullets"][:10]:
                        st.write(f"- {b}")
                st.caption(f"URL: {info['url']}")

        if gap_suggestions:
            st.subheader("ğŸ§© Content Gapï¼ˆTop1 æ²’è¬›ä½†å…¶ä»–äººå¸¸æï¼‰")
            for g in gap_suggestions:
                st.write(f"- {g}")

    # ===== Prompt æ³¨å…¥ï¼ˆç†æ€§å£å»ï¼‰=====
    deep_text_for_prompt = ""
    if deep_briefs:
        deep_text_for_prompt += "\n# ğŸ§  ç«¶å“æ·±åº¦æ‘˜è¦ï¼ˆä½ å·²è®€é Top3 çš„çµæ§‹ï¼‰\n"
        for idx, info in deep_briefs:
            deep_text_for_prompt += (
                f"- #{idx} {domain_of(info['url'])}\n"
                f"  - H1: {safe_str(info.get('h1','ç„¡'))}\n"
                f"  - H2: {', '.join(info.get('h2', [])[:10])}\n"
                f"  - Struct: FAQ={info.get('has_faq',0)}, Table={info.get('has_table',0)}, List={info.get('has_list',0)}\n"
            )

    gap_text = ""
    if gap_suggestions:
        gap_text = "\n# ğŸ§© å»ºè­°è£œå¼·å…§å®¹ç¼ºå£\n" + "\n".join([f"- {g}" for g in gap_suggestions]) + "\n"

    cite_block = "\n# ğŸ“ å»ºè­°å¼•ç”¨æ®µè½ï¼ˆç†æ€§ç‰ˆï¼Œå¯ç›´æ¥ç”¨ï¼‰\n" + rational_paras + "\n"

    st.divider()
    st.subheader("âœï¸ AI æ™ºèƒ½æ–‡æ¡ˆç”Ÿæˆå™¨ï¼ˆç†æ€§ç‰ˆï¼šæœ‰æ•¸æ“šå°±ç”¨å€é–“ã€æœ‰å¼•ç”¨å°±è¬›ä¾†æºï¼‰")

    template_type = st.radio(
        "æ–‡ç« è¦èµ°å“ªç¨®ç†æ€§æ‰“æ³•ï¼Ÿ",
        [
            "âš”ï¸ ç†æ€§ç«¶çˆ­å‹ï¼ˆå°ç…§è¡¨ + ç¼ºå£è£œé½Šï¼‰",
            "ğŸ† ç†æ€§æ¬Šå¨å‹ï¼ˆåˆ¶åº¦/æµç¨‹/å¼•ç”¨å„ªå…ˆï¼‰",
            "ğŸ¤– AI å‹å–„å‹ï¼ˆè¡¨æ ¼ + FAQ + å¯æ‘˜éŒ„ï¼‰"
        ],
        horizontal=True
    )

    if "ç«¶çˆ­å‹" in template_type:
        base_instruction = (
            "è«‹æŠŠå…§å®¹å¯«æˆã€èƒ½è¢«æª¢æ ¸ã€çš„ç‰ˆæœ¬ï¼šä¸»å¼µè¦æœ‰ä¾æ“šã€æ¯”è¼ƒè¦æœ‰è¡¨æ ¼ã€ç¼ºå£è¦è£œå®Œæ•´ã€‚"
        )
        structure_req = (
            "1) é–‹é ­ç”¨ 4â€“6 è¡Œ TL;DRï¼ˆçµè«–å…ˆè¬›ï¼‰\n"
            "2) ç”¨ Markdown è¡¨æ ¼åšã€Œæœ¬æ ¡ vs Top1ã€å°ç…§ï¼ˆèª²ç¨‹/å¯¦ç¿’/è­‰ç…§/å‡ºè·¯/è³‡æºï¼‰\n"
            "3) æŠŠ Content Gap ä¸€æ¬¡è£œé½Šï¼ˆè‡³å°‘ 6 é»ï¼‰\n"
            "4) FAQ è‡³å°‘ 8 é¡Œï¼ˆçŸ­ã€ç›´æ¥ã€å¯æ‘˜éŒ„ï¼‰\n"
        )
    elif "æ¬Šå¨å‹" in template_type:
        base_instruction = (
            "é€™ç¯‡ä»¥ã€åˆ¶åº¦èˆ‡å¯æŸ¥è³‡æ–™ã€å»ºç«‹å¯ä¿¡åº¦ï¼šå…¥å­¸ç®¡é“ã€èª²ç¨‹çµæ§‹ã€å¯¦ç¿’ã€è­‰ç…§/åœ‹è€ƒã€å°±æ¥­è·¯å¾‘ã€‚"
        )
        structure_req = (
            "1) å…¥å­¸ç®¡é“èˆ‡é–€æª»ï¼ˆå¼·èª¿ã€è¿‘ 2â€“3 å¹´å€é–“ã€èˆ‡ä¾†æºï¼‰\n"
            "2) èª²ç¨‹åœ°åœ–èˆ‡å­¸åˆ†çµæ§‹ï¼ˆç”¨è¡¨æ ¼ï¼‰\n"
            "3) å¯¦ç¿’èˆ‡è­‰ç…§/åœ‹è€ƒï¼ˆæµç¨‹åŒ–èªªæ˜ + å¼•ç”¨å»ºè­°ï¼‰\n"
            "4) å‡ºè·¯èˆ‡è–ªè³‡ï¼ˆç”¨å€é–“/å¹´è³‡/è·å‹™ï¼‰\n"
            "5) FAQ è‡³å°‘ 6 é¡Œ\n"
        )
    else:
        base_instruction = (
            "æŠŠæ–‡ç« å¯«æˆ AI æœ€å¥½æ‘˜è¦çš„æ ¼å¼ï¼šçŸ­æ®µè½ã€è¡¨æ ¼ã€æ¢åˆ—ã€FAQï¼Œä¸¦æ¨™ç¤ºå¼•ç”¨ä¾†æºé¡å‹ã€‚"
        )
        structure_req = (
            "1) TL;DRï¼ˆ5 è¡Œï¼‰\n"
            "2) æ ¸å¿ƒè¡¨æ ¼ï¼ˆè‡³å°‘ 1 å¼µï¼‰\n"
            "3) æ­¥é©Ÿæ¸…å–®ï¼ˆé¢è©¦/é¸èª²/è€ƒç…§ä»»ä¸€ï¼‰\n"
            "4) FAQ è‡³å°‘ 10 é¡Œ\n"
        )

    geo_hint = (
        f"- æ„åœ–ï¼š{kw_type}\n"
        f"- æŒ‡æ¨™ï¼šOpportunity={round(float(target_row['Opportunity_Score']),1)}ï½œAI={int(target_row['AI_Potential'])}ï½œ"
        f"Citable={round(float(target_row['Citable_Score']),1)}ï½œAuthority={int(target_row['Authority_Count'])}ï½œForum={int(target_row['Forum_Count'])}\n"
        f"- çµæ§‹åŒ–ï¼šFAQ={int(target_row['Has_FAQ'])}ï½œTable={int(target_row['Has_Table'])}ï½œList={int(target_row['Has_List'])}ï½œH2/H3={int(target_row['Has_Headings'])}\n"
        f"- ç«¶å“å‘½ä¸­ï¼š{safe_str(target_row.get('Competitor_Hit','ç„¡'))}\n"
    )

    final_prompt = f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä½åç†æ€§ã€é‡è¦–å¯æŸ¥è³‡æ–™èˆ‡çµæ§‹åŒ–å‘ˆç¾çš„ SEO + GEO å…§å®¹ç­–ç•¥é¡§å•ã€‚

# ä»»å‹™
ç‚ºã€Œ{dept_name}ã€å¯«ä¸€ç¯‡è¦è¡æ’åã€ä¹Ÿè¦å®¹æ˜“è¢« AI æ‘˜éŒ„/å¼•ç”¨çš„æ–‡ç« ã€‚
ç›®æ¨™é—œéµå­—ï¼š**{kw}**

# ç›®å‰ Top3 åœ¨è¬›ä»€éº¼ï¼ˆæ‘˜è¦ï¼‰
{competitor_info_text}

{deep_text_for_prompt}
{gap_text}
{cite_block}

# æœ¬æ¬¡æˆ°æƒ…å®¤è§€æ¸¬
{geo_hint}

# å¯«ä½œè¦æ±‚ï¼ˆ{template_type}ï¼‰
- èªæ°£åç†æ€§ï¼šé¿å…å£è™Ÿå¼å½¢å®¹è©ï¼Œä¸»å¼µè¦èƒ½è¢«æª¢æ ¸
- æ¶‰åŠæ•¸æ“šï¼ˆè–ªè³‡/åˆ†æ•¸/å­¸åˆ†/åŠæ ¼ç‡ï¼‰å„ªå…ˆç”¨ã€å€é–“ã€ï¼Œä¸¦äº¤ä»£ã€å¹´ä»½/ä¾†æºé¡å‹ã€
- æ–‡ç« è¦ç”¨ Markdownï¼ŒH2/H3 æ¸…æ¥š

# çµæ§‹ï¼ˆç…§åšï¼‰
{structure_req}

# æ”¶å°¾
- å†è£œ 3 é¡Œã€Œå¤§å®¶æœ€å¸¸å•ã€Q&A
- åŠ  CTAï¼šç³»ç¶²/åƒè¨ª/è«®è©¢æ–¹å¼
"""

    st.text_area("ğŸ“‹ è¤‡è£½ Prompt çµ¦ ChatGPT / Gemini / Claudeï¼š", final_prompt, height=620)
    st.success("âœ… è‹¥ä½ æœ‰å•Ÿç”¨æ·±åº¦åˆ†æï¼Œé€™å€‹ Prompt æœƒæ›´åƒã€çœŸçš„çœ‹éå°æ‰‹å…§å®¹ã€å¾Œå¯«å‡ºä¾†çš„ç‰ˆæœ¬ã€‚")

    st.divider()
    st.subheader("ğŸ§¾ æœ¬ç³»é—œéµå­—æ¸…å–®")
    table_cols = [
        "Keyword","Keyword_Type","Opportunity_Score","AI_Potential",
        "Citable_Score","Authority_Count","Forum_Count",
        vcol,"Trends_Fetched","Rank1_Title"
    ]
    table_cols = [c for c in table_cols if c in dept_df.columns]
    st.dataframe(
        dept_df[table_cols].sort_values(["Opportunity_Score","AI_Potential"], ascending=False),
        use_container_width=True,
        height=460
    )


# =========================
# 8) è·¯ç”±
# =========================
if "ç¸½è¦½" in selected_dept:
    if selected_dept == "å…¨æ ¡ç¸½è¦½":
        overview_page(target_df, "å…¨æ ¡")
    else:
        overview_page(target_df, selected_college)
else:
    dept_page(target_df, selected_dept)
