# æª”æ¡ˆåç¨±ï¼š2_dashboard.pyï¼ˆå®Œå…¨å°æ‡‰æœ€æ–°ç‰ˆ powergeo.pyï¼šSource/Evidence + Trends + SERP æ·±åº¦è§£æ + ç†æ€§å¼•ç”¨æ®µè½ï¼‰
import os
import re
import json
import hashlib
from collections import Counter
from urllib.parse import urlparse

import streamlit as st
import pandas as pd
import plotly.express as px

# ---- å¯é¸ï¼šrequests / bs4ï¼ˆæ·±åº¦è§£æç”¨ï¼‰----
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


# =========================
# 0) åŸºæœ¬è¨­å®š
# =========================
st.set_page_config(page_title="å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤", layout="wide")

CACHE_DIR = "serp_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
}

FAQ_HINTS = ["å¸¸è¦‹å•é¡Œ", "FAQ", "å•ç­”", "Q&A", "QA", "å•é¡Œ"]


# =========================
# 1) å°å·¥å…·
# =========================
def safe_str(x, default="ç„¡"):
    if x is None:
        return default
    s = str(x)
    return s if s.strip() else default

def clip_text(s, n=180):
    s = safe_str(s, "")
    return (s[:n] + "â€¦") if len(s) > n else s

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def _dedup_keep_order(items, max_n=10):
    seen = set()
    out = []
    for x in items:
        x = str(x).strip()
        if not x or x in seen:
            continue
        out.append(x)
        seen.add(x)
        if len(out) >= max_n:
            break
    return out

def _to_int_safe(s):
    try:
        return int(s)
    except Exception:
        return None

def _to_float_safe(s):
    try:
        return float(s)
    except Exception:
        return 0.0

def prefer_volume_col(scope_df: pd.DataFrame) -> str:
    """
    å„ªå…ˆç”¨ Trends_Scoreï¼ˆä½ æ–°ç‰ˆ powergeo ä¸»è¦æŒ‡æ¨™ï¼‰ï¼Œæ²’æœ‰å† fallback Search_Volume
    """
    if "Trends_Score" in scope_df.columns and scope_df["Trends_Score"].sum() > 0:
        return "Trends_Score"
    return "Search_Volume"

def source_tag(s: str) -> str:
    s = safe_str(s, "ç„¡").lower()
    if s == "autocomplete":
        return "ğŸ§  Autocomplete"
    if s == "trends_related":
        return "ğŸ“ˆ Trends"
    if s == "serp_mined":
        return "â›ï¸ SERPæŒ–è©"
    if s == "competitor_compare":
        return "âš”ï¸ ç«¶å“æ¯”è¼ƒ"
    if s == "base_template":
        return "ğŸ§© ä¿åº•æ¨¡æ¿"
    if s == "ç„¡":
        return "â€”"
    return s

def cache_key(url: str) -> str:
    return hashlib.md5(url.encode("utf-8")).hexdigest()

def load_cached_page(url: str):
    fp = os.path.join(CACHE_DIR, cache_key(url) + ".json")
    if os.path.exists(fp):
        try:
            with open(fp, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return None
    return None

def save_cached_page(url: str, data: dict):
    fp = os.path.join(CACHE_DIR, cache_key(url) + ".json")
    try:
        with open(fp, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def fetch_html(url: str, timeout=10) -> str:
    if not HAS_REQUESTS:
        return ""
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        ct = (r.headers.get("Content-Type") or "").lower()
        if r.status_code >= 400:
            return ""
        if "text/html" not in ct and "application/xhtml" not in ct:
            return ""
        return r.text or ""
    except Exception:
        return ""


# =========================
# 2) æ·±åº¦åˆ†æï¼šæŠ“é  + çµæ§‹ç‰¹å¾µ + æ•¸å­—ç·šç´¢åˆ†é¡
# =========================
NUM_PATTERN = r"\d+(?:\.\d+)?%?"
MONEY_PATTERN = r"(\d+(?:\.\d+)?)(\s*è¬|\s*å…ƒ|\s*[kK])"
RANGE_PATTERN = r"(\d+(?:\.\d+)?)[\s]*[~ï½\-â€“â€”][\s]*(\d+(?:\.\d+)?)"

KW_SALARY = ["è–ª", "è–ªè³‡", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª", "å¾…é‡", "å…ƒ", "è¬", "k", "K"]
KW_SCORE = ["åˆ†æ•¸", "ç´šåˆ†", "éŒ„å–", "é–€æª»", "æœ€ä½", "çµ±æ¸¬", "ç¹æ˜Ÿ", "ç”„é¸", "è½é»", "PR", "å€ç‡", "ç´šè·"]
KW_CREDITS = ["å­¸åˆ†", "å¿…ä¿®", "é¸ä¿®", "ç¸½å­¸åˆ†", "ç•¢æ¥­å­¸åˆ†", "èª²ç¨‹åœ°åœ–"]
KW_PASS = ["åŠæ ¼", "é€šé", "åˆæ ¼", "åŠæ ¼ç‡", "é€šéç‡", "åˆæ ¼ç‡", "éŒ„å–ç‡", "åœ‹è€ƒ", "è­‰ç…§", "è€ƒç§‘"]

def classify_number_clues(text: str) -> dict:
    clues = {"salary": [], "score": [], "credits": [], "passrate": []}
    if not text:
        return clues
    t = text.replace("ï¼…", "%")

    for m in re.finditer(NUM_PATTERN, t):
        val = m.group(0)
        s = max(0, m.start() - 26)
        e = min(len(t), m.end() + 26)
        ctx = t[s:e].strip()
        if len(ctx) > 95:
            ctx = ctx[:95] + "â€¦"

        if ("%" in val or "%" in ctx) and any(k in ctx for k in KW_PASS):
            clues["passrate"].append(ctx)
            continue
        if any(k in ctx for k in KW_CREDITS) or ("å­¸åˆ†" in ctx):
            clues["credits"].append(ctx)
            continue
        if any(k in ctx for k in KW_SALARY):
            clues["salary"].append(ctx)
            continue
        if any(k in ctx for k in KW_SCORE):
            clues["score"].append(ctx)
            continue

    for k in clues:
        clues[k] = _dedup_keep_order(clues[k], max_n=12)
    return clues

def _extract_money_values(ctx: str):
    vals = []
    for m in re.finditer(MONEY_PATTERN, ctx):
        num = m.group(1)
        unit = m.group(2).strip()
        vals.append((num, unit))
    return vals

def _normalize_money(num_str, unit):
    try:
        x = float(num_str)
    except Exception:
        return None
    u = unit.lower()
    if "è¬" in u:
        return int(x * 10000)
    if "k" in u:
        return int(x * 1000)
    if "å…ƒ" in u:
        return int(x)
    return None

def summarize_salary(clues_salary: list) -> dict:
    if not clues_salary:
        return {"found": False, "type": "ç„¡", "range": None, "points": [], "note": ""}

    types = {"æœˆè–ª": 0, "å¹´è–ª": 0, "èµ·è–ª": 0}
    ranges = []
    points = []

    for ctx in clues_salary[:12]:
        points.append(ctx)
        for t in types:
            if t in ctx:
                types[t] += 1

        rm = re.search(RANGE_PATTERN, ctx)
        if rm and any(k in ctx for k in ["è¬", "å…ƒ", "k", "K", "è–ª", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª"]):
            a, b = rm.group(1), rm.group(2)
            unit = "å…ƒ" if "å…ƒ" in ctx else ("è¬" if "è¬" in ctx else ("k" if ("k" in ctx or "K" in ctx) else "å…ƒ"))
            va = _normalize_money(a, unit)
            vb = _normalize_money(b, unit)
            if va and vb:
                lo, hi = min(va, vb), max(va, vb)
                if 15000 <= lo <= 200000 and 15000 <= hi <= 200000:
                    ranges.append((lo, hi, ctx))

    best_type = max(types, key=lambda k: types[k])
    if types[best_type] == 0:
        best_type = "è–ªè³‡"

    summary_range = None
    if ranges:
        lo, hi, _ = ranges[0]
        summary_range = (lo, hi)

    return {
        "found": True,
        "type": best_type,
        "range": summary_range,
        "points": _dedup_keep_order(points, max_n=6),
        "note": "å»ºè­°ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€æè¿°ï¼Œé¿å…åªä¸Ÿä¸€å€‹å–®é»æ•¸å­—ã€‚"
    }

def summarize_score(clues_score: list) -> dict:
    if not clues_score:
        return {"found": False, "points": [], "note": ""}
    points = _dedup_keep_order(clues_score, max_n=6)
    return {
        "found": True,
        "points": points,
        "note": "é–€æª»æœƒéš¨å¹´åº¦æµ®å‹•ï¼Œæœ€åƒäººæœƒå¯«çš„æ–¹å¼æ˜¯ã€è¿‘ 2â€“3 å¹´å€é–“ã€ï¼‹æ¨™è¨»å…¥å­¸ç®¡é“ï¼‹å¼•ç”¨å®˜æ–¹ç°¡ç« ã€‚"
    }

def summarize_credits(clues_credits: list) -> dict:
    if not clues_credits:
        return {"found": False, "total": None, "required": None, "elective": None, "points": [], "note": ""}

    text = " ".join(clues_credits[:10])
    total = required = elective = None
    m_total = re.search(r"(ç¸½å­¸åˆ†|ç•¢æ¥­å­¸åˆ†)[^\d]{0,6}(\d{2,3})", text)
    if m_total:
        total = _to_int_safe(m_total.group(2))
    m_req = re.search(r"(å¿…ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_req:
        required = _to_int_safe(m_req.group(2))
    m_ele = re.search(r"(é¸ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_ele:
        elective = _to_int_safe(m_ele.group(2))

    return {
        "found": True,
        "total": total,
        "required": required,
        "elective": elective,
        "points": _dedup_keep_order(clues_credits, max_n=6),
        "note": "å­¸åˆ†/èª²ç¨‹ä»¥ç³»ç¶²èª²ç¨‹åœ°åœ–æˆ–èª²ç¨‹æŸ¥è©¢ç³»çµ±ç‚ºæº–ï¼›ç”¨è¡¨æ ¼å‘ˆç¾æœ€æ¸…æ¥šã€‚"
    }

def summarize_passrate(clues_pass: list) -> dict:
    if not clues_pass:
        return {"found": False, "rates": [], "points": [], "note": ""}
    points = _dedup_keep_order(clues_pass, max_n=6)
    rates = []
    for ctx in points:
        for p in re.findall(r"\d+(?:\.\d+)?%", ctx):
            rates.append(p)
    rates = _dedup_keep_order(rates, max_n=6)
    return {
        "found": True,
        "rates": rates,
        "points": points,
        "note": "é€šéç‡/åŠæ ¼ç‡è¦äº¤ä»£ã€å¹´ä»½ã€å£å¾‘ã€æ¯æ•¸ã€ä¸¦æ¨™è¨»ä¾†æºï¼ˆè€ƒé¸éƒ¨/æ ¡æ–¹å…¬é–‹æˆæœï¼‰ã€‚"
    }

def humanize_number_output(agg_clues: dict) -> dict:
    return {
        "salary": summarize_salary(agg_clues.get("salary", [])),
        "score": summarize_score(agg_clues.get("score", [])),
        "credits": summarize_credits(agg_clues.get("credits", [])),
        "passrate": summarize_passrate(agg_clues.get("passrate", [])),
    }

def build_rational_citation_paragraphs(human: dict) -> str:
    paras = []

    sal = human.get("salary", {})
    if sal.get("found"):
        r = sal.get("range")
        if r:
            lo, hi = r
            lo_w = round(lo / 10000, 1)
            hi_w = round(hi / 10000, 1)
            line = f"è–ªè³‡è³‡è¨Šæ¯”è¼ƒåƒäººæœƒå¯«çš„æ–¹å¼æ˜¯ç”¨ã€å€é–“ã€ï¼šå¤§æ¦‚è½åœ¨ **{lo_w}ï½{hi_w} è¬/æœˆ**ï¼ˆæœƒä¾åœ°å€ã€ç­åˆ¥ã€è·å‹™è€Œè®Šå‹•ï¼‰ã€‚"
        else:
            line = "è–ªè³‡è³‡è¨Šå»ºè­°ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€æè¿°ï¼Œé¿å…å–®ä¸€æ•¸å­—é€ æˆèª¤å°ã€‚"

        paras.append(
            "### è–ªè³‡ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            f"{line}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼š104 è·ç¼ºè–ªè³‡å€é–“ã€é†«é™¢/æª¢é©—æ‰€æ‹›å‹Ÿå…¬å‘Šï¼ˆæ¨™å¹´ä»½/ä¾†æºï¼‰ã€‚"
        )

    sc = human.get("score", {})
    if sc.get("found"):
        paras.append(
            "### åˆ†æ•¸/é–€æª»ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            "éŒ„å–é–€æª»æ¯å¹´æœƒå‹•ï¼Œæœ€ç©©çš„å¯«æ³•æ˜¯ï¼š**æ•´ç†è¿‘ 2â€“3 å¹´å€é–“**ï¼Œä¸¦æ¨™è¨»ã€å…¥å­¸ç®¡é“ã€ï¼ˆçµ±æ¸¬åˆ†ç™¼/ç”„é¸/ç¹æ˜Ÿï¼‰ã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šå®˜æ–¹æ‹›ç”Ÿç°¡ç« ã€åˆ†ç™¼/ç”„é¸å…¥å­¸è³‡æ–™ã€‚"
        )

    cr = human.get("credits", {})
    if cr.get("found"):
        t = cr.get("total")
        req = cr.get("required")
        ele = cr.get("elective")
        rows = []
        if t: rows.append(f"- ç•¢æ¥­ç¸½å­¸åˆ†ï¼š{t}")
        if req: rows.append(f"- å¿…ä¿®ï¼š{req}")
        if ele: rows.append(f"- é¸ä¿®ï¼š{ele}")
        detail = "\n".join(rows) if rows else "- å»ºè­°ç›´æ¥è²¼ã€å­¸åˆ†çµæ§‹è¡¨ + èª²ç¨‹åœ°åœ–ã€ï¼Œè®€è€…ä¸€ç§’æ‡‚ã€‚"

        paras.append(
            "### å­¸åˆ†/èª²ç¨‹ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            "èª²ç¨‹è³‡è¨Šç”¨è¡¨æ ¼æœ€æœ‰æ•ˆï¼šæŠŠã€å­¸åˆ†çµæ§‹ã€ï¼‹ã€å¹´ç´šå­¸ç¿’è·¯å¾‘ã€è¬›æ¸…æ¥šã€‚\n"
            f"{detail}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šç³»ç¶²èª²ç¨‹è¦åŠƒã€èª²ç¨‹æŸ¥è©¢ç³»çµ±ã€æ‹›ç”Ÿç°¡ç« é™„éŒ„ã€‚"
        )

    pr = human.get("passrate", {})
    if pr.get("found"):
        rates = pr.get("rates") or []
        rate_line = "ç¶²é ç‰‡æ®µä¸­å¯è¦‹çš„ % åŒ…å«ï¼š" + "ã€".join(rates) + "ï¼ˆä»éœ€æ ¸å°å¹´ä»½èˆ‡å£å¾‘ï¼‰ã€‚" if rates else \
                    "è‹¥è¦å¯«é€šéç‡/åŠæ ¼ç‡ï¼Œå‹™å¿…è£œé½Šå¹´ä»½èˆ‡ä¾†æºï¼Œå¦å‰‡å®¹æ˜“è¢«è³ªç–‘ã€‚"

        paras.append(
            "### åœ‹è€ƒ/è­‰ç…§é€šéç‡ï¼ˆç†æ€§å¯«æ³•ï¼‰\n"
            f"{rate_line}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šè€ƒé¸éƒ¨/å®˜æ–¹å…¬å‘Šã€æ ¡æ–¹å…¬é–‹æˆæœï¼ˆé™„å¹´ä»½ï¼‰ã€‚"
        )

    if not paras:
        return (
            "### å»ºè­°å¼•ç”¨æ®µè½ï¼ˆé€šç”¨ç†æ€§ç‰ˆï¼‰\n"
            "å¦‚æœ Top3 ç¼ºå°‘å¯æŸ¥è­‰æ•¸æ“šï¼Œå»ºè­°ç”¨ã€å®˜æ–¹ä¾†æº + è¡¨æ ¼æ•´ç† + FAQã€è£œé½Šï¼Œæ–‡ç« æœƒæ›´å®¹æ˜“è¢« AI æ‘˜éŒ„ã€‚"
        )

    return "\n\n".join(paras)

def parse_competitor_page(url: str) -> dict:
    cached = load_cached_page(url)
    if cached:
        return cached

    html = fetch_html(url)
    if not html:
        data = {"url": url, "ok": 0, "reason": "fetch_failed"}
        save_cached_page(url, data)
        return data

    if not HAS_BS4:
        # é€€åŒ–ç‰ˆï¼šåªæ‹¿ç´”æ–‡å­—
        text = re.sub(r"<script[\s\S]*?</script>", " ", html, flags=re.I)
        text = re.sub(r"<style[\s\S]*?</style>", " ", text, flags=re.I)
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        has_faq = 1 if any(h.lower() in text.lower() for h in FAQ_HINTS) else 0
        number_clues = classify_number_clues(text)

        data = {
            "url": url, "ok": 1,
            "title": "", "meta_desc": "",
            "h1": "", "h2": [], "h3": [],
            "has_table": 0,
            "has_list": 0,
            "has_faq": has_faq,
            "number_clues": number_clues,
            "bullets": [],
            "text_preview": text[:900],
        }
        save_cached_page(url, data)
        return data

    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    title = soup.title.get_text(strip=True) if soup.title else ""
    meta = soup.find("meta", attrs={"name": "description"})
    meta_desc = meta.get("content", "").strip() if meta else ""

    h1_tag = soup.find("h1")
    h1 = h1_tag.get_text(" ", strip=True) if h1_tag else ""
    h2 = [x.get_text(" ", strip=True) for x in soup.find_all("h2")][:25]
    h3 = [x.get_text(" ", strip=True) for x in soup.find_all("h3")][:25]

    has_table = 1 if soup.find("table") else 0
    has_list = 1 if soup.find(["ul", "ol"]) else 0

    text = soup.get_text(" ", strip=True)
    has_faq = 1 if any(h in text for h in FAQ_HINTS) else 0

    bullets = []
    for ul in soup.find_all(["ul", "ol"])[:3]:
        for li in ul.find_all("li")[:8]:
            t = li.get_text(" ", strip=True)
            if 8 <= len(t) <= 90:
                bullets.append(t)
    bullets = _dedup_keep_order(bullets, max_n=14)

    number_clues = classify_number_clues(text)

    data = {
        "url": url, "ok": 1,
        "title": title,
        "meta_desc": meta_desc,
        "h1": h1,
        "h2": h2,
        "h3": h3,
        "has_table": has_table,
        "has_list": has_list,
        "has_faq": has_faq,
        "number_clues": number_clues,
        "bullets": bullets,
        "text_preview": text[:900],
    }
    save_cached_page(url, data)
    return data


# =========================
# 3) è®€å– CSVï¼ˆå°é½Šæœ€æ–°ç‰ˆ powergeo.pyï¼‰
# =========================
try:
    df = pd.read_csv("school_data.csv")
except FileNotFoundError:
    st.error("âŒ æ‰¾ä¸åˆ° school_data.csvï¼Œè«‹å…ˆåŸ·è¡Œ powergeo.py ç”¢ç”Ÿè³‡æ–™ã€‚")
    st.stop()

# å¿…è¦æ¬„ä½ï¼ˆæ²’æœ‰å°±è£œï¼‰
TEXT_DEFAULTS = {
    "College": "ç„¡",
    "Department": "ç„¡",
    "Keyword": "ç„¡",
    "Keyword_Source": "ç„¡",
    "Seed_Term": "ç„¡",
    "Evidence": "ç„¡",
    "Keyword_Type": "ä¸€èˆ¬",
    "Strategy_Tag": "ç„¡",
    "Rank1_Title": "ç„¡", "Rank1_Link": "#", "Rank1_Snippet": "",
    "Rank2_Title": "ç„¡", "Rank2_Link": "#", "Rank2_Snippet": "",
    "Rank3_Title": "ç„¡", "Rank3_Link": "#", "Rank3_Snippet": "",
}

NUM_DEFAULTS = {
    "Trends_Score": 0.0,
    "Trends_Fetched": 0,
    "Search_Volume": 0,
    "Opportunity_Score": 0.0,
    "AI_Potential": 0,
    "Authority_Count": 0,
    "Forum_Count": 0,
    "Answerable_Avg": 0.0,
    "Citable_Score": 0.0,
    "Fetch_OK_Count": 0,
    "Schema_Hit_Count": 0,
    "Has_FAQ": 0,
    "Has_Table": 0,
    "Has_List": 0,
    "Has_Headings": 0,
    "Page_Word_Count_Max": 0,
    "Result_Count": 0,
}

for c, v in TEXT_DEFAULTS.items():
    if c not in df.columns:
        df[c] = v

for c, v in NUM_DEFAULTS.items():
    if c not in df.columns:
        df[c] = v

# æ¸…ç†å‹åˆ¥
for c in TEXT_DEFAULTS.keys():
    df[c] = df[c].fillna(TEXT_DEFAULTS[c]).astype(str)

for c in NUM_DEFAULTS.keys():
    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(NUM_DEFAULTS[c])

# æ’åºä¸€ä¸‹ï¼ˆçœ‹èµ·ä¾†æ¯”è¼ƒæ­£å¸¸ï¼‰
df = df.sort_values(["College", "Department", "Opportunity_Score"], ascending=[True, True, False])


# =========================
# 4) Sidebarï¼šç¯©é¸ï¼ˆå®Œå…¨å°æ‡‰æ–°ç‰ˆï¼‰
# =========================
st.sidebar.title("ğŸ« å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤")

college_list = ["å…¨éƒ¨å­¸é™¢"] + sorted(df["College"].unique().tolist())
selected_college = st.sidebar.selectbox("STEP 1: é¸æ“‡å­¸é™¢", college_list)

if selected_college == "å…¨éƒ¨å­¸é™¢":
    dept_options = ["å…¨æ ¡ç¸½è¦½"] + sorted(df["Department"].unique().tolist())
else:
    dept_options = ["å­¸é™¢ç¸½è¦½"] + sorted(df[df["College"] == selected_college]["Department"].unique().tolist())

selected_dept = st.sidebar.selectbox("STEP 2: é¸æ“‡ç§‘ç³»/è¦–è§’", dept_options)

kw_types = ["å…¨éƒ¨æ„åœ–"] + sorted(df["Keyword_Type"].unique().tolist())
selected_kw_type = st.sidebar.selectbox("STEP 3: ç¯©é¸æœå°‹æ„åœ–", kw_types)

source_list = ["å…¨éƒ¨ä¾†æº"] + sorted(df["Keyword_Source"].unique().tolist())
selected_source = st.sidebar.selectbox("STEP 4: ç¯©é¸ Keyword ä¾†æº", source_list)

min_ai = st.sidebar.slider("AI_Potential æœ€ä½é–€æª»", 0, 100, 0, 5)
min_opp_max = int(max(1, df["Opportunity_Score"].max()))
min_opp = st.sidebar.slider("Opportunity_Score æœ€ä½é–€æª»", 0, min_opp_max, 0, 10)

st.sidebar.caption("æç¤ºï¼šæƒ³çœ‹æœ€åƒçœŸäººè¼¸å…¥çš„ï¼Œä¾†æºé¸ Autocompleteã€‚")


# å¥—ç”¨ç¯©é¸
target_df = df.copy()
if selected_college != "å…¨éƒ¨å­¸é™¢":
    target_df = target_df[target_df["College"] == selected_college]

if selected_kw_type != "å…¨éƒ¨æ„åœ–":
    target_df = target_df[target_df["Keyword_Type"] == selected_kw_type]

if selected_source != "å…¨éƒ¨ä¾†æº":
    target_df = target_df[target_df["Keyword_Source"] == selected_source]

target_df = target_df[target_df["AI_Potential"] >= min_ai]
target_df = target_df[target_df["Opportunity_Score"] >= min_opp]


# =========================
# 5) ç¸½è¦½é 
# =========================
def overview_page(scope_df: pd.DataFrame, title_prefix: str):
    st.title(f"ğŸ“Š {title_prefix}ï¼šGEO/AI æˆ°ç•¥åœ°åœ–ï¼ˆå°é½Š powergeo æ–°ç‰ˆï¼‰")

    vcol = prefer_volume_col(scope_df)
    vlabel = "Google Trends ç›¸å°è²é‡" if vcol == "Trends_Score" else "è²é‡æŒ‡æ¨™"

    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: st.metric("é—œéµå­—ç­†æ•¸", int(len(scope_df)))
    with c2: st.metric("å¹³å‡ Opportunity", round(scope_df["Opportunity_Score"].mean(), 1) if len(scope_df) else 0)
    with c3: st.metric("å¹³å‡ AI_Potential", round(scope_df["AI_Potential"].mean(), 1) if len(scope_df) else 0)
    with c4: st.metric("å¹³å‡ Citable", round(scope_df["Citable_Score"].mean(), 1) if len(scope_df) else 0)
    with c5: st.metric(f"å¹³å‡ {vlabel}", round(scope_df[vcol].mean(), 2) if len(scope_df) else 0)

    st.divider()

    left, right = st.columns([2, 1])
    with left:
        dept_rank = (
            scope_df.groupby("Department", as_index=False)["Opportunity_Score"]
            .mean()
            .sort_values("Opportunity_Score", ascending=False)
        )
        fig = px.bar(dept_rank, x="Department", y="Opportunity_Score", color="Department",
                     title="å„ç³» GEO æ©Ÿæœƒå€¼æ’è¡Œï¼ˆå¹³å‡ Opportunityï¼‰")
        st.plotly_chart(fig, use_container_width=True)

    with right:
        fig2 = px.pie(scope_df, names="Keyword_Type", title="æœå°‹æ„åœ–åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)

    st.divider()

    colA, colB = st.columns(2)
    with colA:
        vol_rank = (
            scope_df.groupby("Department", as_index=False)[vcol]
            .mean()
            .sort_values(vcol, ascending=False)
        )
        fig3 = px.bar(vol_rank, x="Department", y=vcol, color="Department",
                      title=f"å„ç³» {vlabel}ï¼ˆå¹³å‡ï¼‰")
        st.plotly_chart(fig3, use_container_width=True)

    with colB:
        src_rank = (
            scope_df.groupby("Keyword_Source", as_index=False)
            .size()
            .rename(columns={"size": "Count"})
            .sort_values("Count", ascending=False)
        )
        fig4 = px.bar(src_rank, x="Keyword_Source", y="Count", color="Keyword_Source",
                      title="Keyword ä¾†æºåˆ†ä½ˆï¼ˆçœ‹æ˜¯ä¸æ˜¯ template ç”¢çš„ï¼‰")
        st.plotly_chart(fig4, use_container_width=True)

    st.divider()
    st.subheader("ğŸ“‹ é—œéµå­—ç¸½è¡¨ï¼ˆå«ä¾†æºèˆ‡è­‰æ“šï¼‰")

    show_cols = [
        "College","Department","Keyword","Keyword_Source","Seed_Term",
        "Keyword_Type","Opportunity_Score","AI_Potential","Citable_Score",
        "Authority_Count","Forum_Count", vcol, "Trends_Fetched", "Rank1_Title"
    ]
    show_cols = [c for c in show_cols if c in scope_df.columns]

    st.dataframe(
        scope_df[show_cols].sort_values(["Opportunity_Score","AI_Potential"], ascending=False),
        use_container_width=True,
        height=620
    )


# =========================
# 6) å–®ä¸€ç§‘ç³»é ï¼šTop3 + æ·±åº¦è§£æ + Prompt æ³¨å…¥ï¼ˆä¾†æºè­‰æ“šï¼‰
# =========================
def dept_page(scope_df: pd.DataFrame, dept_name: str):
    st.title(f"ğŸ” {dept_name}ï¼šç«¶å“ + ä¾†æºè­‰æ“š + æ·±åº¦è§£æ + ç†æ€§ Prompt")

    dept_df = scope_df[scope_df["Department"] == dept_name].copy()
    if dept_df.empty:
        st.warning("é€™å€‹ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰è³‡æ–™ã€‚å¯ä»¥æŠŠå·¦é‚Šé–€æª»èª¿ä½ä¸€é»å†çœ‹ã€‚")
        st.stop()

    dept_df = dept_df.sort_values(["Opportunity_Score","AI_Potential"], ascending=False)
    vcol = prefer_volume_col(dept_df)
    vlabel = "Trends ç›¸å°è²é‡" if vcol == "Trends_Score" else "è²é‡æŒ‡æ¨™"

    k1, k2, k3, k4, k5 = st.columns(5)
    with k1: st.metric("é—œéµå­—ç­†æ•¸", int(len(dept_df)))
    with k2: st.metric("å¹³å‡ Opportunity", round(dept_df["Opportunity_Score"].mean(), 1))
    with k3: st.metric("å¹³å‡ AI_Potential", round(dept_df["AI_Potential"].mean(), 1))
    with k4: st.metric("å¹³å‡ Citable", round(dept_df["Citable_Score"].mean(), 1))
    with k5: st.metric(f"å¹³å‡ {vlabel}", round(dept_df[vcol].mean(), 2))

    st.divider()

    colX, colY = st.columns([2, 1])
    with colX:
        fig = px.box(dept_df, x="Keyword_Source", y="Opportunity_Score", title="ä¸åŒä¾†æºçš„æ©Ÿæœƒå€¼åˆ†ä½ˆ")
        st.plotly_chart(fig, use_container_width=True)
    with colY:
        fig2 = px.bar(
            dept_df.groupby("Keyword_Source", as_index=False)["AI_Potential"].mean().sort_values("AI_Potential", ascending=False),
            x="Keyword_Source", y="AI_Potential", title="ä¸åŒä¾†æºå¹³å‡ AI_Potential"
        )
        st.plotly_chart(fig2, use_container_width=True)

    st.divider()
    st.subheader("ğŸ•µï¸ é¸ä¸€å€‹é—œéµå­—ï¼Œçœ‹ Top3 +ï¼ˆå¯é¸ï¼‰æ·±åº¦è§£æ")

    dept_df["Display_Label"] = (
        dept_df["Keyword"] + " ã€”" +
        dept_df["Keyword_Type"] + " / " +
        dept_df["Keyword_Source"].apply(source_tag) + "ã€•"
    )
    target_label = st.selectbox("è«‹é¸æ“‡é—œéµå­—", dept_df["Display_Label"].unique())
    target_row = dept_df[dept_df["Display_Label"] == target_label].iloc[0]

    kw = safe_str(target_row["Keyword"])
    kw_type = safe_str(target_row["Keyword_Type"])
    strategy = safe_str(target_row["Strategy_Tag"])
    src = safe_str(target_row["Keyword_Source"])
    seed = safe_str(target_row["Seed_Term"])
    evidence = safe_str(target_row.get("Evidence", "ç„¡"))

    st.caption(f"ä¾†æºï¼š{source_tag(src)}ï½œSeedï¼š{seed}ï½œæ„åœ–ï¼š{kw_type}")

    if evidence != "ç„¡" and evidence.strip():
        with st.expander("ğŸ” Evidenceï¼ˆç‚ºä»€éº¼èªªé€™ä¸æ˜¯ä½ ç·¨çš„ï¼‰", expanded=False):
            st.code(evidence[:600])

    deep_on = False
    run_deep = False
    if HAS_REQUESTS:
        deep_on = st.checkbox("å•Ÿç”¨æ·±åº¦è§£æï¼šæŠ“ Top3 ç¶²é ï¼ˆç¬¬ä¸€æ¬¡æ…¢ã€æœ‰å¿«å–ï¼‰", value=False)
        run_deep = st.button("é–‹å§‹æ·±åº¦è§£æ Top3")
    else:
        st.info("ï¼ˆå¯é¸ï¼‰è‹¥è¦æ·±åº¦è§£æ Top3 ç¶²é ï¼šè«‹å…ˆ pip install requests beautifulsoup4")

    st.divider()

    # å·¦å´ï¼šæŒ‡æ¨™
    col_l, col_r = st.columns([1, 2])
    with col_l:
        st.metric("Opportunity", round(float(target_row["Opportunity_Score"]), 1))
        st.metric("AI_Potential", int(target_row["AI_Potential"]))
        st.metric("Citable", round(float(target_row["Citable_Score"]), 1))
        st.metric(vlabel, round(float(target_row.get(vcol, 0)), 2))
        st.metric("Authority", int(target_row["Authority_Count"]))
        st.metric("Forum", int(target_row["Forum_Count"]))

        st.caption("çµæ§‹åŒ–ç‰¹å¾µï¼ˆè¶Šå¤šè¶Šå®¹æ˜“è¢« AI æ‘˜éŒ„ï¼‰")
        s_cols = st.columns(4)
        s_cols[0].metric("FAQ", int(target_row["Has_FAQ"]))
        s_cols[1].metric("Table", int(target_row["Has_Table"]))
        s_cols[2].metric("List", int(target_row["Has_List"]))
        s_cols[3].metric("H2/H3", int(target_row["Has_Headings"]))

        st.info(f"ç­–ç•¥å»ºè­°ï¼š{strategy}")

    # å³å´ï¼šTop3 + æ·±åº¦è§£æçµæœ
    competitor_info_text = ""
    deep_briefs = []
    gap_pool_h2 = []
    agg_number_clues = {"salary": [], "score": [], "credits": [], "passrate": []}

    with col_r:
        st.markdown(f"### ğŸ‘€ ã€Œ{kw}ã€Top 3 æœå°‹çµæœ")
        for i in range(1, 4):
            title = safe_str(target_row.get(f"Rank{i}_Title", "ç„¡"))
            link = safe_str(target_row.get(f"Rank{i}_Link", "#"))
            snippet = safe_str(target_row.get(f"Rank{i}_Snippet", ""))

            if title == "ç„¡":
                continue

            competitor_info_text += f"{i}. æ¨™é¡Œï¼š{title}\n   æ‘˜è¦ï¼š{clip_text(snippet, 140)}\n"

            with st.container(border=True):
                st.markdown(f"**#{i} [{title}]({link})**")
                if snippet.strip():
                    st.caption(clip_text(snippet, 260))

            if deep_on and run_deep and link not in ["#", "ç„¡", ""]:
                info = parse_competitor_page(link)
                if info.get("ok") == 1:
                    deep_briefs.append((i, info))
                    gap_pool_h2.extend(info.get("h2", [])[:15])

                    nc = info.get("number_clues", {}) or {}
                    for k in agg_number_clues.keys():
                        agg_number_clues[k].extend(nc.get(k, []))

    # Content Gap + ç†æ€§å¼•ç”¨æ®µè½
    gap_suggestions = []
    if deep_briefs:
        top1_h2 = set(deep_briefs[0][1].get("h2", []))
        freq = Counter([h for h in gap_pool_h2 if 4 <= len(h) <= 24])
        for h, _ in freq.most_common(12):
            if h not in top1_h2:
                gap_suggestions.append(h)
        gap_suggestions = gap_suggestions[:8]

    for k in agg_number_clues:
        agg_number_clues[k] = _dedup_keep_order(agg_number_clues[k], max_n=12)

    human = humanize_number_output(agg_number_clues)
    rational_paras = build_rational_citation_paragraphs(human)

    if deep_on and run_deep:
        st.divider()
        st.subheader("ğŸ“Œ æ·±åº¦è§£ææ‘˜è¦ï¼ˆæŠŠæ•¸å­—ç·šç´¢è®Šæˆã€äººé¡æœƒå¯«ã€çš„æ®µè½ï¼‰")
        with st.container(border=True):
            st.markdown(rational_paras)

        if gap_suggestions:
            st.subheader("ğŸ§© Content Gapï¼ˆTop1 æ²’è¬›ã€ä½†å…¶ä»–äººå¸¸æï¼‰")
            for g in gap_suggestions:
                st.write(f"- {g}")

        st.subheader("ğŸ§¾ ç«¶å“é é¢çµæ§‹æ‘˜è¦ï¼ˆå°ç…§ç”¨ï¼‰")
        for idx, info in deep_briefs:
            with st.expander(f"#{idx} {domain_of(info['url'])}ï½œ{info.get('title','')[:60]}"):
                st.write(f"**H1ï¼š** {safe_str(info.get('h1','ç„¡'))}")
                if info.get("meta_desc"):
                    st.write(f"**Metaï¼š** {info.get('meta_desc')}")
                st.write(f"**çµæ§‹åŒ–ï¼š** FAQ={info.get('has_faq',0)}ï½œTable={info.get('has_table',0)}ï½œList={info.get('has_list',0)}")
                if info.get("h2"):
                    st.write("**H2ï¼š** " + " / ".join(info["h2"][:12]))
                if info.get("bullets"):
                    st.write("**æ¢åˆ—ï¼š**")
                    for b in info["bullets"][:10]:
                        st.write(f"- {b}")
                st.caption(f"URL: {info['url']}")

    # Prompt æ³¨å…¥å€
    st.divider()
    st.subheader("âœï¸ AI æ™ºèƒ½æ–‡æ¡ˆç”Ÿæˆå™¨ï¼ˆå®Œå…¨å°æ‡‰ powergeo æ–°ç‰ˆï¼šä¾†æºè­‰æ“š + ç†æ€§å¼•ç”¨æ®µè½ï¼‰")

    template_type = st.radio(
        "æ–‡ç« è¦èµ°å“ªç¨®ç†æ€§æ‰“æ³•ï¼Ÿ",
        [
            "âš”ï¸ ç†æ€§ç«¶çˆ­å‹ï¼ˆå°ç…§è¡¨ + ç¼ºå£è£œé½Šï¼‰",
            "ğŸ† ç†æ€§æ¬Šå¨å‹ï¼ˆæµç¨‹/åˆ¶åº¦/å¼•ç”¨å„ªå…ˆï¼‰",
            "ğŸ¤– AI å‹å–„å‹ï¼ˆè¡¨æ ¼ + FAQ + å¯æ‘˜éŒ„ï¼‰"
        ],
        horizontal=True
    )

    if "ç«¶çˆ­å‹" in template_type:
        base_instruction = "è«‹æŠŠå…§å®¹å¯«æˆã€èƒ½è¢«æª¢æ ¸ã€çš„ç‰ˆæœ¬ï¼šä¸»å¼µè¦æœ‰ä¾æ“šã€æ¯”è¼ƒè¦æœ‰è¡¨æ ¼ã€ç¼ºå£è¦è£œå®Œæ•´ã€‚"
        structure_req = (
            "1) é–‹é ­ç”¨ 4â€“6 è¡Œ TL;DRï¼ˆçµè«–å…ˆè¬›ï¼‰\n"
            "2) Markdown è¡¨æ ¼ï¼šæœ¬æ ¡ vs Top1ï¼ˆèª²ç¨‹/å¯¦ç¿’/è­‰ç…§/å‡ºè·¯/è³‡æºï¼‰\n"
            "3) Content Gap ä¸€æ¬¡è£œé½Šï¼ˆè‡³å°‘ 6 é»ï¼‰\n"
            "4) FAQ è‡³å°‘ 8 é¡Œï¼ˆçŸ­ã€ç›´æ¥ã€å¯æ‘˜éŒ„ï¼‰\n"
        )
    elif "æ¬Šå¨å‹" in template_type:
        base_instruction = "é€™ç¯‡ä»¥ã€åˆ¶åº¦èˆ‡å¯æŸ¥è³‡æ–™ã€å»ºç«‹å¯ä¿¡åº¦ï¼šå…¥å­¸ç®¡é“ã€èª²ç¨‹çµæ§‹ã€å¯¦ç¿’ã€è­‰ç…§/åœ‹è€ƒã€å°±æ¥­è·¯å¾‘ã€‚"
        structure_req = (
            "1) å…¥å­¸ç®¡é“èˆ‡é–€æª»ï¼ˆå¼·èª¿ã€è¿‘ 2â€“3 å¹´å€é–“ã€èˆ‡ä¾†æºï¼‰\n"
            "2) èª²ç¨‹åœ°åœ–èˆ‡å­¸åˆ†çµæ§‹ï¼ˆç”¨è¡¨æ ¼ï¼‰\n"
            "3) å¯¦ç¿’èˆ‡è­‰ç…§/åœ‹è€ƒï¼ˆæµç¨‹åŒ–èªªæ˜ + å¼•ç”¨å»ºè­°ï¼‰\n"
            "4) å‡ºè·¯èˆ‡è–ªè³‡ï¼ˆç”¨å€é–“/å¹´è³‡/è·å‹™ï¼‰\n"
            "5) FAQ è‡³å°‘ 6 é¡Œ\n"
        )
    else:
        base_instruction = "æŠŠæ–‡ç« å¯«æˆ AI æœ€å¥½æ‘˜è¦çš„æ ¼å¼ï¼šçŸ­æ®µè½ã€è¡¨æ ¼ã€æ¢åˆ—ã€FAQï¼Œä¸¦æ¨™ç¤ºå¼•ç”¨ä¾†æºé¡å‹ã€‚"
        structure_req = (
            "1) TL;DRï¼ˆ5 è¡Œï¼‰\n"
            "2) æ ¸å¿ƒè¡¨æ ¼ï¼ˆè‡³å°‘ 1 å¼µï¼‰\n"
            "3) æ­¥é©Ÿæ¸…å–®ï¼ˆé¢è©¦/é¸èª²/è€ƒç…§ä»»ä¸€ï¼‰\n"
            "4) FAQ è‡³å°‘ 10 é¡Œ\n"
        )

    deep_text_for_prompt = ""
    if deep_briefs:
        deep_text_for_prompt += "\n# ğŸ§  ç«¶å“æ·±åº¦æ‘˜è¦ï¼ˆä½ å·²è®€é Top3 çš„çµæ§‹ï¼‰\n"
        for idx, info in deep_briefs:
            deep_text_for_prompt += (
                f"- #{idx} {domain_of(info['url'])}\n"
                f"  - H1: {safe_str(info.get('h1','ç„¡'))}\n"
                f"  - H2: {', '.join(info.get('h2', [])[:10])}\n"
                f"  - Struct: FAQ={info.get('has_faq',0)}, Table={info.get('has_table',0)}, List={info.get('has_list',0)}\n"
            )

    gap_text = ""
    if gap_suggestions:
        gap_text = "\n# ğŸ§© å»ºè­°è£œå¼·å…§å®¹ç¼ºå£\n" + "\n".join([f"- {g}" for g in gap_suggestions]) + "\n"

    cite_block = "\n# ğŸ“ å»ºè­°å¼•ç”¨æ®µè½ï¼ˆç†æ€§ç‰ˆï¼Œå¯ç›´æ¥è²¼ï¼‰\n" + rational_paras + "\n"

    geo_hint = (
        f"- æ„åœ–ï¼š{kw_type}\n"
        f"- æŒ‡æ¨™ï¼šOpportunity={round(float(target_row['Opportunity_Score']),1)}ï½œAI={int(target_row['AI_Potential'])}ï½œ"
        f"Citable={round(float(target_row['Citable_Score']),1)}ï½œAuthority={int(target_row['Authority_Count'])}ï½œForum={int(target_row['Forum_Count'])}\n"
        f"- çµæ§‹åŒ–ï¼šFAQ={int(target_row['Has_FAQ'])}ï½œTable={int(target_row['Has_Table'])}ï½œList={int(target_row['Has_List'])}ï½œH2/H3={int(target_row['Has_Headings'])}\n"
        f"- ä¾†æºï¼š{src}ï½œSeedï¼š{seed}\n"
    )

    # é—œéµï¼šæŠŠã€Œä¾†æºè­‰æ“šã€å¯«æˆæ–‡ç« ä¸€å¥è©±çš„äº¤ä»£ï¼ˆå¾ˆåƒäººæœƒåšçš„ï¼‰
    source_explain = (
        "è«‹åœ¨æ–‡ç« å‰æ®µç”¨ä¸€å¥è©±äº¤ä»£è³‡æ–™ä¾†æºèˆ‡å£å¾‘ï¼Œè®“è®€è€…çŸ¥é“ä½ ä¸æ˜¯äº‚ç·¨ã€‚\n"
        "ä¾‹å¦‚ï¼šã€æœ¬æ–‡é—œéµå­—ä¾†è‡ª Google å»ºè­°è©/Trends çš„ç›¸é—œæŸ¥è©¢ï¼Œå†å°ç…§ç›®å‰ Top3 å…§å®¹è£œè¶³ç¼ºå£ï¼›æ¶‰åŠè–ªè³‡/é–€æª»/é€šéç‡æœƒä»¥å®˜æ–¹æˆ–å¯æŸ¥ä¾†æºç‚ºæº–ã€‚ã€"
    )

    final_prompt = f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä½åç†æ€§ã€é‡è¦–å¯æŸ¥è³‡æ–™èˆ‡çµæ§‹åŒ–å‘ˆç¾çš„ SEO + GEO å…§å®¹ç­–ç•¥é¡§å•ã€‚

# ä»»å‹™
ç‚ºã€Œ{dept_name}ã€å¯«ä¸€ç¯‡è¦è¡æ’åã€ä¹Ÿè¦å®¹æ˜“è¢« AI æ‘˜éŒ„/å¼•ç”¨çš„æ–‡ç« ã€‚
ç›®æ¨™é—œéµå­—ï¼š**{kw}**

# é—œéµå­—ä¾†æºï¼ˆè¦æé«˜å¯ä¿¡åº¦ï¼Œè«‹åœ¨æ–‡ä¸­äº¤ä»£ï¼‰
- Keyword_Sourceï¼š{src}
- Seed_Termï¼š{seed}
- Evidenceï¼š{evidence}

# ç›®å‰ Top3 åœ¨è¬›ä»€éº¼ï¼ˆæ‘˜è¦ï¼‰
{competitor_info_text}
{deep_text_for_prompt}
{gap_text}
{cite_block}

# æœ¬æ¬¡æˆ°æƒ…å®¤è§€æ¸¬
{geo_hint}

# å¯«ä½œç­–ç•¥
{base_instruction}
{source_explain}

# çµæ§‹ï¼ˆç…§åšï¼‰
{structure_req}

# Constraints
- ç”¨ Markdownï¼ˆH2/H3 æ¸…æ¥šï¼Œè¡¨æ ¼è¦èƒ½è¢«å¿«é€Ÿæƒè®€ï¼‰
- èªæ°£åç†æ€§ï¼šé¿å…å£è™Ÿå¼å½¢å®¹è©ï¼Œä¸»å¼µè¦èƒ½è¢«æª¢æ ¸
- æ¶‰åŠæ•¸æ“šï¼ˆè–ªè³‡/åˆ†æ•¸/å­¸åˆ†/åŠæ ¼ç‡ï¼‰å„ªå…ˆç”¨ã€å€é–“ã€ï¼Œä¸¦äº¤ä»£ã€å¹´ä»½/ä¾†æºé¡å‹ã€
- æ–‡æœ«è£œ 3 é¡Œã€Œå¤§å®¶æœ€å¸¸å•ã€Q&A + CTAï¼ˆç³»ç¶²/åƒè¨ª/è«®è©¢ï¼‰
"""

    st.text_area("ğŸ“‹ è¤‡è£½ Prompt çµ¦ ChatGPT / Gemini / Claudeï¼š", final_prompt, height=680)
    st.success("âœ… é€™ä»½ Prompt å·²æŠŠã€Keyword ä¾†æºè­‰æ“š + ç†æ€§å¼•ç”¨æ®µè½ + Top3 æ‘˜è¦/ç¼ºå£ã€ä¸€å£æ°£å¡é€²å»ï¼Œæ–‡ç« æœƒæ›´åƒçœŸçš„åšéè³‡æ–™ã€‚")

    st.divider()
    st.subheader("ğŸ§¾ æœ¬ç³»é—œéµå­—æ¸…å–®ï¼ˆå«ä¾†æºï¼‰")
    table_cols = [
        "Keyword","Keyword_Source","Seed_Term","Keyword_Type",
        "Opportunity_Score","AI_Potential","Citable_Score",
        "Authority_Count","Forum_Count", vcol, "Trends_Fetched", "Rank1_Title"
    ]
    table_cols = [c for c in table_cols if c in dept_df.columns]
    st.dataframe(
        dept_df[table_cols].sort_values(["Opportunity_Score","AI_Potential"], ascending=False),
        use_container_width=True,
        height=520
    )


# =========================
# 7) è·¯ç”±ï¼šç¸½è¦½ / å–®ç§‘ç³»
# =========================
if "ç¸½è¦½" in selected_dept:
    if selected_dept == "å…¨æ ¡ç¸½è¦½":
        overview_page(target_df, "å…¨æ ¡")
    else:
        overview_page(target_df, selected_college)
else:
    dept_page(target_df, selected_dept)
