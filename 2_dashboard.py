# æª”æ¡ˆåç¨±ï¼š2_dashboard.py
# å‡ç´šç‰ˆï¼šå®Œå…¨å°æ‡‰æœ€æ–°ç‰ˆ powergeo.pyï¼ˆKeyword_Source / Seed_Term / Evidence / Trends_Score...ï¼‰
# æ–°å¢ï¼šç³»ä¸»ä»»ä¸€é å¼ï¼ˆç«¶å“Top5ã€æ±ºç­–å•é¡ŒTop10ã€å…§å®¹ç¼ºå£ã€ä¸‹æœˆè¡Œå‹•æ¸…å–®ï¼‰+ åŸæˆ°æƒ…å®¤ + Prompt æ³¨å…¥

import os
import re
import json
import hashlib
from collections import Counter
from urllib.parse import urlparse

import streamlit as st
import pandas as pd
import plotly.express as px

# ---- å¯é¸ï¼šrequests / bs4ï¼ˆæ·±åº¦è§£æç”¨ï¼‰----
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


# =========================
# 0) åŸºæœ¬è¨­å®š
# =========================
st.set_page_config(page_title="å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤", layout="wide")

CACHE_DIR = "serp_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
}

FAQ_HINTS = ["å¸¸è¦‹å•é¡Œ", "FAQ", "å•ç­”", "Q&A", "QA", "å•é¡Œ"]
SELF_BRAND_TOKENS = ["ä¸­è¯é†«äº‹", "è¯é†«", "ä¸­è¯é†«äº‹ç§‘æŠ€å¤§å­¸"]


# =========================
# 1) å·¥å…·å‡½æ•¸
# =========================
def safe_str(x, default="ç„¡"):
    if x is None:
        return default
    s = str(x)
    return s if s.strip() else default

def clip_text(s, n=180):
    s = safe_str(s, "")
    return (s[:n] + "â€¦") if len(s) > n else s

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def _dedup_keep_order(items, max_n=10):
    seen = set()
    out = []
    for x in items:
        x = str(x).strip()
        if not x or x in seen:
            continue
        out.append(x)
        seen.add(x)
        if len(out) >= max_n:
            break
    return out

def _to_int_safe(s):
    try:
        return int(s)
    except Exception:
        return None

def _to_float_safe(s):
    try:
        return float(s)
    except Exception:
        return 0.0

def prefer_volume_col(scope_df: pd.DataFrame) -> str:
    """å„ªå…ˆç”¨ Trends_Scoreï¼ˆæ–°ç‰ˆä¸»è¦æŒ‡æ¨™ï¼‰ï¼Œæ²’æœ‰å† fallback Search_Volume"""
    if "Trends_Score" in scope_df.columns and scope_df["Trends_Score"].sum() > 0:
        return "Trends_Score"
    return "Search_Volume"

def source_tag(s: str) -> str:
    s = safe_str(s, "ç„¡").lower()
    if s == "autocomplete":
        return "ğŸ§  Autocomplete"
    if s == "trends_related":
        return "ğŸ“ˆ Trends"
    if s == "serp_mined":
        return "â›ï¸ SERPæŒ–è©"
    if s == "competitor_compare":
        return "âš”ï¸ ç«¶å“æ¯”è¼ƒ"
    if s == "base_template":
        return "ğŸ§© ä¿åº•æ¨¡æ¿"
    if s == "ç„¡":
        return "â€”"
    return s


# =========================
# 2) å¯é¸å¤–éƒ¨çœŸå¯¦æ•¸æ“šï¼ˆä¸ç”¨ä¹Ÿèƒ½è·‘ï¼‰
#    ä½ æœªä¾†è‹¥æœ‰ GA4/è¡¨å–®/æ´»å‹•æ•¸æ“šï¼Œæ”¾åŒè³‡æ–™å¤¾å°±æœƒè‡ªå‹•åƒé€²ä¾†
# =========================
# funnel_data.csv å»ºè­°æ¬„ä½ï¼ˆä»»é¸ï¼‰ï¼šDepartment, Exposure, Click, Lead, Visit, Enroll
FUNNEL_FILE = "funnel_data.csv"
# gsc_queries.csv å»ºè­°æ¬„ä½ï¼ˆä»»é¸ï¼‰ï¼šDepartment, Query, Impressions, Clicks, Position
GSC_FILE = "gsc_queries.csv"

def load_optional_csv(path: str):
    if os.path.exists(path):
        try:
            return pd.read_csv(path)
        except Exception:
            return None
    return None

funnel_df = load_optional_csv(FUNNEL_FILE)
gsc_df = load_optional_csv(GSC_FILE)


# =========================
# 3) æ·±åº¦è§£æï¼ˆå¯é¸ï¼‰ï¼šæŠ“ Top3 é é¢ï¼ŒæŒ–ã€Œæ•¸å­—ç·šç´¢ã€èˆ‡ã€Œçµæ§‹ã€
# =========================
def cache_key(url: str) -> str:
    return hashlib.md5(url.encode("utf-8")).hexdigest()

def load_cached_page(url: str):
    fp = os.path.join(CACHE_DIR, cache_key(url) + ".json")
    if os.path.exists(fp):
        try:
            with open(fp, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return None
    return None

def save_cached_page(url: str, data: dict):
    fp = os.path.join(CACHE_DIR, cache_key(url) + ".json")
    try:
        with open(fp, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def fetch_html(url: str, timeout=10) -> str:
    if not HAS_REQUESTS:
        return ""
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        ct = (r.headers.get("Content-Type") or "").lower()
        if r.status_code >= 400:
            return ""
        if "text/html" not in ct and "application/xhtml" not in ct:
            return ""
        return r.text or ""
    except Exception:
        return ""

NUM_PATTERN = r"\d+(?:\.\d+)?%?"
MONEY_PATTERN = r"(\d+(?:\.\d+)?)(\s*è¬|\s*å…ƒ|\s*[kK])"
RANGE_PATTERN = r"(\d+(?:\.\d+)?)[\s]*[~ï½\-â€“â€”][\s]*(\d+(?:\.\d+)?)"

KW_SALARY = ["è–ª", "è–ªè³‡", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª", "å¾…é‡", "å…ƒ", "è¬", "k", "K"]
KW_SCORE = ["åˆ†æ•¸", "ç´šåˆ†", "éŒ„å–", "é–€æª»", "æœ€ä½", "çµ±æ¸¬", "ç¹æ˜Ÿ", "ç”„é¸", "è½é»", "PR", "å€ç‡", "ç´šè·"]
KW_CREDITS = ["å­¸åˆ†", "å¿…ä¿®", "é¸ä¿®", "ç¸½å­¸åˆ†", "ç•¢æ¥­å­¸åˆ†", "èª²ç¨‹åœ°åœ–", "èª²è¡¨"]
KW_PASS = ["åŠæ ¼", "é€šé", "åˆæ ¼", "åŠæ ¼ç‡", "é€šéç‡", "åˆæ ¼ç‡", "éŒ„å–ç‡", "åœ‹è€ƒ", "è­‰ç…§", "è€ƒç§‘"]

def classify_number_clues(text: str) -> dict:
    clues = {"salary": [], "score": [], "credits": [], "passrate": []}
    if not text:
        return clues
    t = text.replace("ï¼…", "%")

    for m in re.finditer(NUM_PATTERN, t):
        val = m.group(0)
        s = max(0, m.start() - 26)
        e = min(len(t), m.end() + 26)
        ctx = t[s:e].strip()
        if len(ctx) > 95:
            ctx = ctx[:95] + "â€¦"

        if ("%" in val or "%" in ctx) and any(k in ctx for k in KW_PASS):
            clues["passrate"].append(ctx)
            continue
        if any(k in ctx for k in KW_CREDITS) or ("å­¸åˆ†" in ctx):
            clues["credits"].append(ctx)
            continue
        if any(k in ctx for k in KW_SALARY):
            clues["salary"].append(ctx)
            continue
        if any(k in ctx for k in KW_SCORE):
            clues["score"].append(ctx)
            continue

    for k in clues:
        clues[k] = _dedup_keep_order(clues[k], max_n=12)
    return clues

def _normalize_money(num_str, unit):
    try:
        x = float(num_str)
    except Exception:
        return None
    u = unit.lower()
    if "è¬" in u:
        return int(x * 10000)
    if "k" in u:
        return int(x * 1000)
    if "å…ƒ" in u:
        return int(x)
    return None

def summarize_salary(clues_salary: list) -> dict:
    if not clues_salary:
        return {"found": False, "type": "ç„¡", "range": None, "points": [], "note": ""}

    types = {"æœˆè–ª": 0, "å¹´è–ª": 0, "èµ·è–ª": 0}
    ranges = []
    points = []

    for ctx in clues_salary[:12]:
        points.append(ctx)
        for t in types:
            if t in ctx:
                types[t] += 1

        rm = re.search(RANGE_PATTERN, ctx)
        if rm and any(k in ctx for k in ["è¬", "å…ƒ", "k", "K", "è–ª", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª"]):
            a, b = rm.group(1), rm.group(2)
            unit = "å…ƒ" if "å…ƒ" in ctx else ("è¬" if "è¬" in ctx else ("k" if ("k" in ctx or "K" in ctx) else "å…ƒ"))
            va = _normalize_money(a, unit)
            vb = _normalize_money(b, unit)
            if va and vb:
                lo, hi = min(va, vb), max(va, vb)
                if 15000 <= lo <= 200000 and 15000 <= hi <= 200000:
                    ranges.append((lo, hi, ctx))

    best_type = max(types, key=lambda k: types[k])
    if types[best_type] == 0:
        best_type = "è–ªè³‡"

    summary_range = None
    if ranges:
        lo, hi, _ = ranges[0]
        summary_range = (lo, hi)

    return {
        "found": True,
        "type": best_type,
        "range": summary_range,
        "points": _dedup_keep_order(points, max_n=6),
        "note": "ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€å¯«æ³•æœ€åƒäººï¼Œä¹Ÿæœ€ä¸å®¹æ˜“è¢«è³ªç–‘ã€‚"
    }

def summarize_score(clues_score: list) -> dict:
    if not clues_score:
        return {"found": False, "points": [], "note": ""}
    points = _dedup_keep_order(clues_score, max_n=6)
    return {
        "found": True,
        "points": points,
        "note": "é–€æª»æœƒæµ®å‹•ï¼Œæœ€ç©©çš„å¯«æ³•æ˜¯ã€è¿‘ 2â€“3 å¹´å€é–“ã€ï¼‹æ¨™è¨»å…¥å­¸ç®¡é“ï¼‹å¼•ç”¨å®˜æ–¹ç°¡ç« ã€‚"
    }

def summarize_credits(clues_credits: list) -> dict:
    if not clues_credits:
        return {"found": False, "total": None, "required": None, "elective": None, "points": [], "note": ""}

    text = " ".join(clues_credits[:10])
    total = required = elective = None
    m_total = re.search(r"(ç¸½å­¸åˆ†|ç•¢æ¥­å­¸åˆ†)[^\d]{0,6}(\d{2,3})", text)
    if m_total:
        total = _to_int_safe(m_total.group(2))
    m_req = re.search(r"(å¿…ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_req:
        required = _to_int_safe(m_req.group(2))
    m_ele = re.search(r"(é¸ä¿®)[^\d]{0,6}(\d{2,3})", text)
    if m_ele:
        elective = _to_int_safe(m_ele.group(2))

    return {
        "found": True,
        "total": total,
        "required": required,
        "elective": elective,
        "points": _dedup_keep_order(clues_credits, max_n=6),
        "note": "å­¸åˆ†/èª²ç¨‹ç”¨ã€èª²ç¨‹åœ°åœ– + è¡¨æ ¼ã€å‘ˆç¾æœ€æœ‰æ•ˆï¼Œä¸¦æ¨™è¨»ä¾†æºï¼ˆç³»ç¶²/èª²ç¨‹ç³»çµ±ï¼‰ã€‚"
    }

def summarize_passrate(clues_pass: list) -> dict:
    if not clues_pass:
        return {"found": False, "rates": [], "points": [], "note": ""}
    points = _dedup_keep_order(clues_pass, max_n=6)
    rates = []
    for ctx in points:
        for p in re.findall(r"\d+(?:\.\d+)?%", ctx):
            rates.append(p)
    rates = _dedup_keep_order(rates, max_n=6)
    return {
        "found": True,
        "rates": rates,
        "points": points,
        "note": "é€šéç‡/åŠæ ¼ç‡è¦äº¤ä»£ã€å¹´ä»½ã€å£å¾‘ã€æ¯æ•¸ã€ä¸¦æ¨™è¨»ä¾†æºï¼ˆè€ƒé¸éƒ¨/æ ¡æ–¹å…¬é–‹æˆæœï¼‰ã€‚"
    }

def humanize_number_output(agg_clues: dict) -> dict:
    return {
        "salary": summarize_salary(agg_clues.get("salary", [])),
        "score": summarize_score(agg_clues.get("score", [])),
        "credits": summarize_credits(agg_clues.get("credits", [])),
        "passrate": summarize_passrate(agg_clues.get("passrate", [])),
    }

def build_rational_citation_paragraphs(human: dict) -> str:
    paras = []

    sal = human.get("salary", {})
    if sal.get("found"):
        r = sal.get("range")
        if r:
            lo, hi = r
            lo_w = round(lo / 10000, 1)
            hi_w = round(hi / 10000, 1)
            line = f"è–ªè³‡ä¸è¦å¯«æˆå–®é»ï¼šæ¯”è¼ƒåƒäººæœƒå¯«çš„æ–¹å¼æ˜¯ã€å€é–“ã€ï¼Œå¤§æ¦‚ **{lo_w}ï½{hi_w} è¬/æœˆ**ï¼ˆä¾åœ°å€ã€ç­åˆ¥ã€è·å‹™è€Œå‹•ï¼‰ã€‚"
        else:
            line = "è–ªè³‡å»ºè­°ç”¨ã€å€é–“ + å¹´è³‡/è·å‹™ã€æè¿°ï¼Œé¿å…å–®ä¸€æ•¸å­—é€ æˆèª¤è§£ã€‚"

        paras.append(
            "### è–ªè³‡ï¼ˆå»ºè­°å¼•ç”¨æ®µè½ï¼‰\n"
            f"{line}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼š104 è·ç¼ºè–ªè³‡å€é–“ã€é†«é™¢/æ©Ÿæ§‹å¾µæ‰å…¬å‘Šï¼ˆè¨»æ˜å¹´ä»½/è·å‹™ï¼‰ã€‚"
        )

    sc = human.get("score", {})
    if sc.get("found"):
        paras.append(
            "### åˆ†æ•¸/é–€æª»ï¼ˆå»ºè­°å¼•ç”¨æ®µè½ï¼‰\n"
            "éŒ„å–é–€æª»æ¯å¹´æœƒå‹•ï¼Œæœ€ç©©çš„å¯«æ³•æ˜¯ï¼š**æ•´ç†è¿‘ 2â€“3 å¹´å€é–“**ï¼Œä¸¦æ¨™è¨»ã€å…¥å­¸ç®¡é“ã€ï¼ˆçµ±æ¸¬åˆ†ç™¼/ç”„é¸/ç¹æ˜Ÿï¼‰ã€‚\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šå®˜æ–¹æ‹›ç”Ÿç°¡ç« ã€åˆ†ç™¼/ç”„é¸å…¥å­¸å…¬å‘Šã€‚"
        )

    cr = human.get("credits", {})
    if cr.get("found"):
        t = cr.get("total")
        req = cr.get("required")
        ele = cr.get("elective")
        rows = []
        if t: rows.append(f"- ç•¢æ¥­ç¸½å­¸åˆ†ï¼š{t}")
        if req: rows.append(f"- å¿…ä¿®ï¼š{req}")
        if ele: rows.append(f"- é¸ä¿®ï¼š{ele}")
        detail = "\n".join(rows) if rows else "- å»ºè­°ç›´æ¥è²¼ã€å­¸åˆ†çµæ§‹è¡¨ + èª²ç¨‹åœ°åœ–ã€ï¼Œè®€è€…æœƒæ›´å®‰å¿ƒã€‚"

        paras.append(
            "### å­¸åˆ†/èª²ç¨‹ï¼ˆå»ºè­°å¼•ç”¨æ®µè½ï¼‰\n"
            "èª²ç¨‹è³‡è¨Šç”¨è¡¨æ ¼æœ€æ¸…æ¥šï¼šæŠŠã€å­¸åˆ†çµæ§‹ã€ï¼‹ã€å¹´ç´šå­¸ç¿’è·¯å¾‘ã€è¬›æ¸…æ¥šã€‚\n"
            f"{detail}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šç³»ç¶²èª²ç¨‹è¦åŠƒã€èª²ç¨‹æŸ¥è©¢ç³»çµ±ã€æ‹›ç”Ÿç°¡ç« é™„éŒ„ã€‚"
        )

    pr = human.get("passrate", {})
    if pr.get("found"):
        rates = pr.get("rates") or []
        rate_line = "ç‰‡æ®µå‡ºç¾çš„ % åŒ…å«ï¼š" + "ã€".join(rates) + "ï¼ˆä»éœ€æ ¸å°å¹´ä»½èˆ‡å£å¾‘ï¼‰ã€‚" if rates else \
                    "è‹¥è¦å¯«é€šéç‡/åŠæ ¼ç‡ï¼Œå‹™å¿…è£œé½Šå¹´ä»½èˆ‡ä¾†æºï¼Œå¦å‰‡å®¹æ˜“è¢«è³ªç–‘ã€‚"

        paras.append(
            "### åœ‹è€ƒ/è­‰ç…§é€šéç‡ï¼ˆå»ºè­°å¼•ç”¨æ®µè½ï¼‰\n"
            f"{rate_line}\n"
            "- **å¼•ç”¨å»ºè­°**ï¼šè€ƒé¸éƒ¨/å®˜æ–¹å…¬å‘Šã€æ ¡æ–¹å…¬é–‹æˆæœï¼ˆé™„å¹´ä»½/æ¯æ•¸ï¼‰ã€‚"
        )

    if not paras:
        return (
            "### å»ºè­°å¼•ç”¨æ®µè½ï¼ˆé€šç”¨ï¼‰\n"
            "å¦‚æœ Top3 ç¼ºå°‘å¯æŸ¥è­‰æ•¸æ“šï¼Œå»ºè­°ç”¨ã€å®˜æ–¹ä¾†æº + è¡¨æ ¼æ•´ç† + FAQã€è£œé½Šï¼Œæ–‡ç« æ›´å®¹æ˜“è¢« AI æ‘˜éŒ„ã€‚"
        )

    return "\n\n".join(paras)

@st.cache_data(show_spinner=False)
def parse_competitor_page(url: str) -> dict:
    cached = load_cached_page(url)
    if cached:
        return cached

    html = fetch_html(url)
    if not html:
        data = {"url": url, "ok": 0, "reason": "fetch_failed"}
        save_cached_page(url, data)
        return data

    # æ²’ bs4 â†’ é€€åŒ–ç‰ˆ
    if not HAS_BS4:
        text = re.sub(r"<script[\s\S]*?</script>", " ", html, flags=re.I)
        text = re.sub(r"<style[\s\S]*?</style>", " ", text, flags=re.I)
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        has_faq = 1 if any(h.lower() in text.lower() for h in FAQ_HINTS) else 0
        number_clues = classify_number_clues(text)

        data = {
            "url": url, "ok": 1,
            "title": "", "meta_desc": "",
            "h1": "", "h2": [], "h3": [],
            "has_table": 0,
            "has_list": 0,
            "has_faq": has_faq,
            "number_clues": number_clues,
            "bullets": [],
            "text_preview": text[:900],
        }
        save_cached_page(url, data)
        return data

    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    title = soup.title.get_text(strip=True) if soup.title else ""
    meta = soup.find("meta", attrs={"name": "description"})
    meta_desc = meta.get("content", "").strip() if meta else ""

    h1_tag = soup.find("h1")
    h1 = h1_tag.get_text(" ", strip=True) if h1_tag else ""
    h2 = [x.get_text(" ", strip=True) for x in soup.find_all("h2")][:25]
    h3 = [x.get_text(" ", strip=True) for x in soup.find_all("h3")][:25]

    has_table = 1 if soup.find("table") else 0
    has_list = 1 if soup.find(["ul", "ol"]) else 0

    text = soup.get_text(" ", strip=True)
    has_faq = 1 if any(h in text for h in FAQ_HINTS) else 0

    bullets = []
    for ul in soup.find_all(["ul", "ol"])[:3]:
        for li in ul.find_all("li")[:8]:
            t = li.get_text(" ", strip=True)
            if 8 <= len(t) <= 90:
                bullets.append(t)
    bullets = _dedup_keep_order(bullets, max_n=14)

    number_clues = classify_number_clues(text)

    data = {
        "url": url, "ok": 1,
        "title": title,
        "meta_desc": meta_desc,
        "h1": h1,
        "h2": h2,
        "h3": h3,
        "has_table": has_table,
        "has_list": has_list,
        "has_faq": has_faq,
        "number_clues": number_clues,
        "bullets": bullets,
        "text_preview": text[:900],
    }
    save_cached_page(url, data)
    return data


# =========================
# 4) è®€å– school_data.csvï¼ˆå°é½Šæ–°ç‰ˆ powergeo.pyï¼‰
# =========================
try:
    df = pd.read_csv("school_data.csv")
except FileNotFoundError:
    st.error("âŒ æ‰¾ä¸åˆ° school_data.csvï¼Œè«‹å…ˆåŸ·è¡Œ powergeo.py ç”¢ç”Ÿè³‡æ–™ã€‚")
    st.stop()

TEXT_DEFAULTS = {
    "College": "ç„¡",
    "Department": "ç„¡",
    "Keyword": "ç„¡",
    "Keyword_Source": "ç„¡",
    "Seed_Term": "ç„¡",
    "Evidence": "ç„¡",
    "Keyword_Type": "ä¸€èˆ¬",
    "Strategy_Tag": "ç„¡",
    "Rank1_Title": "ç„¡", "Rank1_Link": "#", "Rank1_Snippet": "",
    "Rank2_Title": "ç„¡", "Rank2_Link": "#", "Rank2_Snippet": "",
    "Rank3_Title": "ç„¡", "Rank3_Link": "#", "Rank3_Snippet": "",
}

NUM_DEFAULTS = {
    "Trends_Score": 0.0,
    "Trends_Fetched": 0,
    "Search_Volume": 0,
    "Opportunity_Score": 0.0,
    "AI_Potential": 0,
    "Authority_Count": 0,
    "Forum_Count": 0,
    "Answerable_Avg": 0.0,
    "Citable_Score": 0.0,
    "Fetch_OK_Count": 0,
    "Schema_Hit_Count": 0,
    "Has_FAQ": 0,
    "Has_Table": 0,
    "Has_List": 0,
    "Has_Headings": 0,
    "Page_Word_Count_Max": 0,
    "Result_Count": 0,
}

for c, v in TEXT_DEFAULTS.items():
    if c not in df.columns:
        df[c] = v

for c, v in NUM_DEFAULTS.items():
    if c not in df.columns:
        df[c] = v

for c in TEXT_DEFAULTS.keys():
    df[c] = df[c].fillna(TEXT_DEFAULTS[c]).astype(str)

for c in NUM_DEFAULTS.keys():
    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(NUM_DEFAULTS[c])

df = df.sort_values(["College", "Department", "Opportunity_Score"], ascending=[True, True, False])


# =========================
# 5) å¾ SERP Title æŠ½ã€Œå­¸æ ¡åã€â†’ ç«¶å“Top5
# =========================
SCHOOL_SUFFIX = r"(?:å¤§å­¸|ç§‘æŠ€å¤§å­¸|é†«å­¸é™¢|å­¸é™¢|å°ˆç§‘å­¸æ ¡|è­·ç†å¥åº·å¤§å­¸|è­·ç†å°ˆç§‘å­¸æ ¡|é†«è­·ç®¡ç†å°ˆç§‘å­¸æ ¡|è­·å°ˆ|é†«å°ˆ)"
SCHOOL_REGEX = re.compile(rf"([\u4e00-\u9fff]{{2,12}}{SCHOOL_SUFFIX})")

def extract_school_names(text: str):
    if not text:
        return []
    found = SCHOOL_REGEX.findall(text)
    out = []
    for f in found:
        f = f.strip()
        if not f:
            continue
        # æ’é™¤è‡ªå·±
        if any(t in f for t in SELF_BRAND_TOKENS):
            continue
        out.append(f)
    return out

def competitor_top5_from_dept(dept_df: pd.DataFrame):
    counter = Counter()
    examples = {}

    for _, r in dept_df.iterrows():
        for i in range(1, 4):
            t = safe_str(r.get(f"Rank{i}_Title", ""))
            link = safe_str(r.get(f"Rank{i}_Link", "#"))
            d = domain_of(link)

            for name in extract_school_names(t):
                counter[name] += 2
                examples.setdefault(name, []).append(t)

            # domain ä¹Ÿç®—ç«¶å“ç·šç´¢ï¼ˆæ²’æŠ“åˆ°æ ¡åæ™‚ï¼‰
            if d and d not in ["#", ""]:
                counter[d] += 1
                examples.setdefault(d, []).append(t)

    # æŠŠæ˜é¡¯ä¸æ˜¯ç«¶å“çš„ domain å…ˆå£“æ‰
    noise_domains = ["dcard.tw", "ptt.cc", "facebook.com", "youtube.com", "104.com.tw", "instagram.com"]
    for nd in noise_domains:
        for k in list(counter.keys()):
            if nd in k:
                counter[k] -= 2

    items = []
    for name, cnt in counter.most_common(20):
        if cnt <= 0:
            continue
        # æ’é™¤æœ¬æ ¡å­—æ¨£
        if any(t in name for t in SELF_BRAND_TOKENS):
            continue
        items.append({
            "Competitor": name,
            "Mentions": int(cnt),
            "Example_Title": clip_text(examples.get(name, [""])[0], 90)
        })

    # åªå– Top5ï¼ˆé¿å…å¤ªé›œï¼‰
    return items[:5]


# =========================
# 6) å­¸ç”Ÿæ±ºç­–å•é¡Œ Top10ï¼ˆå¾ Keyword/Evidence ä¾†ï¼‰
#    ç›®çš„ï¼šç³»ä¸»ä»»è¦çœ‹åˆ°ã€Œå­¸ç”Ÿç‚ºä»€éº¼é¸/ä¸é¸ã€çš„åŸæ–‡å•é¡Œé¡å‹
# =========================
CAT_RULES = {
    "è–ªè³‡": ["è–ª", "è–ªè³‡", "æœˆè–ª", "å¹´è–ª", "èµ·è–ª", "å¾…é‡", "å¤šå°‘éŒ¢", "å¹¾è¬", "k", "K"],
    "åˆ†æ•¸": ["åˆ†æ•¸", "ç´šåˆ†", "éŒ„å–", "é–€æª»", "æœ€ä½", "çµ±æ¸¬", "ç¹æ˜Ÿ", "ç”„é¸", "è½é»", "å€ç‡", "PR"],
    "å­¸åˆ†": ["å­¸åˆ†", "èª²ç¨‹", "èª²è¡¨", "å¿…ä¿®", "é¸ä¿®", "ç•¢æ¥­å­¸åˆ†", "èª²ç¨‹åœ°åœ–"],
    "åŠæ ¼ç‡": ["åŠæ ¼ç‡", "é€šéç‡", "åˆæ ¼ç‡", "åœ‹è€ƒ", "è­‰ç…§", "è€ƒç§‘", "é€šé", "åŠæ ¼", "åˆæ ¼"],
    "å¯¦ç¿’": ["å¯¦ç¿’", "é†«é™¢", "æ©Ÿæ§‹", "è‡¨åºŠ", "è¦‹ç¿’", "è¼ªè¨“"],
    "å‡ºè·¯": ["å‡ºè·¯", "å·¥ä½œå…§å®¹", "å¥½æ‰¾å·¥ä½œ", "å°±æ¥­", "è·å‹™", "èƒ½åšä»€éº¼", "è·æ¶¯"],
    "ç”Ÿæ´»": ["å®¿èˆ", "ç§Ÿå±‹", "äº¤é€š", "é€šå‹¤", "å­¸è²»", "çå­¸é‡‘", "æ‰“å·¥", "ç”Ÿæ´»è²»"],
    "ç¤¾ç¾¤ç–‘æ…®": ["dcard", "ptt", "é åŒ—", "å¿ƒå¾—", "è©•åƒ¹", "å¾ˆç´¯", "çˆ†è‚", "å¾Œæ‚”", "é›·"],
}

QUESTION_TOKENS = ["å—", "æ€éº¼", "å¦‚ä½•", "è¦ä¸è¦", "å€¼å¾—", "å¥½ä¸å¥½", "é›£ä¸é›£", "æœƒä¸æœƒ", "é©åˆ", "å¯ä»¥"]

def categorize_question(q: str):
    ql = q.lower()
    for cat, keys in CAT_RULES.items():
        if any(k.lower() in ql for k in keys):
            return cat
    return "å…¶ä»–"

def looks_like_question(q: str):
    if any(tok in q for tok in QUESTION_TOKENS):
        return True
    # ä¹ŸæŠŠã€ŒX åˆ†æ•¸ã€ã€ŒX è–ªæ°´ã€é€™ç¨®ç®—å•é¡Œï¼ˆæ±ºç­–å‹ï¼‰
    if any(k in q for k in ["åˆ†æ•¸", "é–€æª»", "éŒ„å–", "è–ªæ°´", "èµ·è–ª", "å¹´è–ª", "åœ‹è€ƒ", "åŠæ ¼ç‡", "å­¸åˆ†", "å¯¦ç¿’"]):
        return True
    return False

def decision_questions_top10(dept_df: pd.DataFrame):
    """
    å„ªå…ˆç”¨ Keyword_Source=autocompleteï¼Œå› ç‚ºæœ€åƒçœŸäººè¼¸å…¥ï¼›å†è£œå…¶ä»–ä¾†æº
    """
    qs = []

    ac = dept_df[dept_df["Keyword_Source"].str.lower() == "autocomplete"]
    other = dept_df[dept_df["Keyword_Source"].str.lower() != "autocomplete"]

    for _, r in pd.concat([ac, other], axis=0).iterrows():
        kw = safe_str(r["Keyword"])
        if looks_like_question(kw):
            qs.append(kw)

    # é »ç‡
    counter = Counter([q.strip() for q in qs if q.strip()])
    top = counter.most_common(30)

    # åš category å½™æ•´ + æ¯é¡æŒ‘ä»£è¡¨å¥
    cat_counter = Counter()
    cat_examples = {}
    for q, cnt in top:
        cat = categorize_question(q)
        cat_counter[cat] += cnt
        cat_examples.setdefault(cat, []).append(q)

    # Top10 å•é¡Œï¼ˆåŸå¥ï¼‰
    top10 = [{"Question": q, "Count": int(cnt), "Category": categorize_question(q)} for q, cnt in counter.most_common(10)]

    # åˆ†é¡è¡¨
    cat_rows = []
    total = sum(cat_counter.values()) if cat_counter else 0
    for cat, cnt in cat_counter.most_common(10):
        ex = cat_examples.get(cat, [])
        cat_rows.append({
            "Category": cat,
            "Share": round((cnt / total) * 100, 1) if total else 0.0,
            "Example": clip_text(ex[0], 80) if ex else "â€”"
        })

    return top10, cat_rows


# =========================
# 7) å…§å®¹ç¼ºå£ + ä¸‹æœˆè¡Œå‹•æ¸…å–®ï¼ˆè®“ç³»ä¸»ä»»èƒ½åšäº‹ï¼‰
# =========================
def content_gap_suggestions(dept_df: pd.DataFrame):
    """
    ç”¨ä½ ç¾æœ‰æ¬„ä½å…ˆåšã€å¯è¡Œå‹•ã€ç¼ºå£ï¼›è‹¥æœ‰æ·±åº¦è§£æï¼Œé‚„æœƒå†åŠ å¼·
    """
    # ç”¨å¹³å‡å€¼çœ‹æ•´é«”å¼±é»
    faq_rate = dept_df["Has_FAQ"].mean() if len(dept_df) else 0
    table_rate = dept_df["Has_Table"].mean() if len(dept_df) else 0
    list_rate = dept_df["Has_List"].mean() if len(dept_df) else 0
    authority = dept_df["Authority_Count"].mean() if len(dept_df) else 0
    forum = dept_df["Forum_Count"].mean() if len(dept_df) else 0
    citable = dept_df["Citable_Score"].mean() if len(dept_df) else 0

    gaps = []

    # çµæ§‹åŒ–ç¼ºå£
    if faq_rate < 0.4:
        gaps.append("FAQ æ²’åšæ»¿ï¼šè£œä¸€æ®µã€å¸¸è¦‹å•é¡Œ 8â€“12 é¡Œã€ï¼Œæ¯é¡Œ 2â€“4 è¡Œï¼ŒAI å¾ˆæ„›æ‘˜ã€‚")
    if table_rate < 0.35:
        gaps.append("ç¼ºå°‘è¡¨æ ¼ï¼šè‡³å°‘åš 1 å¼µã€èª²ç¨‹/å¯¦ç¿’/è­‰ç…§/å‡ºè·¯ã€æ•´ç†è¡¨æˆ–å°ç…§è¡¨ã€‚")
    if list_rate < 0.5:
        gaps.append("ç¼ºå°‘æ­¥é©ŸåŒ–æ¸…å–®ï¼šæŠŠã€å¦‚ä½•æº–å‚™/å¦‚ä½•å¯¦ç¿’/å¦‚ä½•è€ƒç…§ã€å¯«æˆ 6â€“10 æ­¥é©Ÿã€‚")

    # å¼•ç”¨ç¼ºå£
    if citable < 45 or authority < 0.8:
        gaps.append("å¼•ç”¨ä¸è¶³ï¼šæ¶‰åŠè–ªè³‡/é–€æª»/é€šéç‡ï¼Œå‹™å¿…é™„ã€å¹´ä»½+ä¾†æºé¡å‹ã€ï¼ˆå®˜æ–¹/104/æ‹›ç”Ÿç°¡ç« ï¼‰ã€‚")

    # ç¤¾ç¾¤é¢¨éšª
    if forum >= 0.7:
        gaps.append("è«–å£‡å æ¯”åé«˜ï¼šåŠ ä¸€æ®µã€ç†æ€§æ¾„æ¸…ã€ï¼ŒæŠŠä¸»è§€æŠ±æ€¨è½‰æˆå¯æŸ¥è³‡è¨Šï¼ˆæµç¨‹/å£å¾‘/FAQï¼‰ã€‚")

    # æ±ºç­–å››å¤§ç¡¬é¡Œï¼ˆæ°¸é è¦æœ‰ï¼‰
    must_have = [
        "è–ªè³‡ï¼šç”¨ã€å€é–“ + è·å‹™/å¹´è³‡ã€å¯«æ³•ï¼Œä¸è¦å–®ä¸€æ•¸å­—ã€‚",
        "é–€æª»ï¼šæ•´ç†ã€è¿‘ 2â€“3 å¹´å€é–“ã€ï¼‹å…¥å­¸ç®¡é“ï¼‹å¼•ç”¨ç°¡ç« ã€‚",
        "å­¸åˆ†ï¼šè²¼ã€å­¸åˆ†çµæ§‹è¡¨ + èª²ç¨‹åœ°åœ–ã€ã€‚",
        "åŠæ ¼ç‡/è€ƒç…§ï¼šäº¤ä»£ã€å¹´ä»½ã€å£å¾‘ã€æ¯æ•¸ã€ä¸¦é™„ä¾†æºã€‚"
    ]
    gaps.extend(must_have)

    return _dedup_keep_order(gaps, max_n=10)

def next_30_days_action_plan(dept_df: pd.DataFrame, top_questions: list, top_competitors: list):
    """
    ç”¨è¦å‰‡ç”¢ç”Ÿç³»ä¸»ä»»çœ‹å¾—æ‡‚çš„è¡Œå‹•æ¸…å–®ï¼ˆä½ ä¸ç”¨å…ˆæœ‰ GA4 ä¹Ÿèƒ½å…ˆè·‘ï¼‰
    """
    actions = []

    # 1) ç›´æ¥å°æ‡‰å­¸ç”Ÿæœ€å¸¸å•
    cats = Counter([x.get("Category") for x in top_questions])
    top_cat = cats.most_common(1)[0][0] if cats else "å…¶ä»–"

    if top_cat in ["è–ªè³‡", "åˆ†æ•¸", "å­¸åˆ†", "åŠæ ¼ç‡"]:
        actions.append(f"åšä¸€ç¯‡ã€{top_cat} ä¸€æ¬¡è¬›æ¸…æ¥šã€ï¼šç”¨è¡¨æ ¼æ•´ç† + åœ¨æ–‡ä¸­äº¤ä»£å¹´ä»½/ä¾†æºå£å¾‘ã€‚")
    else:
        actions.append("åšä¸€ç¯‡ã€æ–°ç”Ÿæ‡¶äººåŒ…ã€ï¼šèª²ç¨‹åœ°åœ–ã€å¯¦ç¿’æµç¨‹ã€è­‰ç…§/å‡ºè·¯ã€FAQ ä¸€æ¬¡åˆ°ä½ã€‚")

    # 2) ç«¶å“å°ç…§
    if top_competitors:
        actions.append("åšä¸€å¼µã€æœ¬ç³» vs ä¸»è¦ç«¶å“ã€å°ç…§è¡¨ï¼ˆèª²ç¨‹/å¯¦ç¿’/è­‰ç…§/å‡ºè·¯/è³‡æºï¼‰ï¼Œæ”¾åœ¨æ–‡ç« å‰åŠæ®µã€‚")

    # 3) FAQ/å¯æ‘˜éŒ„
    actions.append("è£œ FAQ 12 é¡Œï¼šç›´æ¥ç”¨å­¸ç”Ÿçš„åŸå¥æ”¹å¯«ï¼Œç­”æ¡ˆæ§åˆ¶ 2â€“4 è¡Œï¼Œæ–¹ä¾¿ AI æ‘˜éŒ„ã€‚")

    # 4) å¼•ç”¨è³‡æ–™ç›¤é»
    actions.append("ç›¤é»å¯å¼•ç”¨è³‡æ–™æ¸…å–®ï¼šæ‹›ç”Ÿç°¡ç« ï¼ˆé–€æª»/ç®¡é“ï¼‰ã€èª²ç¨‹åœ°åœ–ï¼ˆå­¸åˆ†ï¼‰ã€å¯¦ç¿’å–®ä½ã€è­‰ç…§/åœ‹è€ƒæˆæœã€å°±æ¥­/è–ªè³‡ä½è­‰ã€‚")

    # 5) ç¤¾ç¾¤é¢¨éšªè™•ç†
    if dept_df["Forum_Count"].mean() >= 0.7:
        actions.append("åŠ ã€ç†æ€§æ¾„æ¸…ã€æ®µï¼šé‡å° Dcard/PTT å¸¸è¦‹ç„¦æ…®ï¼ˆç´¯ä¸ç´¯/å¥½ä¸å¥½è€ƒ/å€¼ä¸å€¼å¾—ï¼‰é€é»å›ç­”ã€‚")

    return _dedup_keep_order(actions, max_n=6)

def build_onepager_markdown(dept_name: str, snapshot: dict, comp_items: list, cat_rows: list, top10_q: list, gaps: list, actions: list):
    md = []
    md.append(f"# {dept_name}ï½œç³»ä¸»ä»»ä¸€é å¼ï¼ˆæ‹›ç”Ÿæ±ºç­–ä¾æ“šï¼‰")
    md.append("")
    md.append("## 1) æ‹›ç”Ÿå¿«ç…§")
    md.append(f"- é—œéµå­—ç­†æ•¸ï¼š{snapshot.get('n',0)}")
    md.append(f"- å¹³å‡ Opportunityï¼š{snapshot.get('opp',0)}ï½œå¹³å‡ AIï¼š{snapshot.get('ai',0)}ï½œå¹³å‡ Citableï¼š{snapshot.get('citable',0)}")
    md.append(f"- å¹³å‡è²é‡æŒ‡æ¨™ï¼š{snapshot.get('vol_label','')} = {snapshot.get('vol',0)}")
    md.append("")
    md.append("## 2) ä¸»è¦ç«¶å“ Top5ï¼ˆä¾†è‡ª Top3 SERP æ¨™é¡Œ/ç¶²åŸŸï¼‰")
    for x in comp_items:
        md.append(f"- {x['Competitor']}ï¼ˆæåŠ {x['Mentions']}ï¼‰ä¾‹ï¼š{x['Example_Title']}")
    md.append("")
    md.append("## 3) å­¸ç”Ÿæ±ºç­–å•é¡Œï¼ˆåˆ†é¡ï¼‰")
    for r in cat_rows:
        md.append(f"- {r['Category']}ï¼š{r['Share']}%ï½œä¾‹ï¼š{r['Example']}")
    md.append("")
    md.append("## 4) Top10 åŸå¥å•é¡Œï¼ˆæœ€åƒå­¸ç”ŸçœŸçš„æœƒå•çš„ï¼‰")
    for q in top10_q:
        md.append(f"- [{q['Category']}] {q['Question']}ï¼ˆ{q['Count']}ï¼‰")
    md.append("")
    md.append("## 5) å…§å®¹ç¼ºå£ï¼ˆç¾åœ¨ç¶²è·¯ä¸Šå®¹æ˜“ç¼ºçš„ï¼‰")
    for g in gaps:
        md.append(f"- {g}")
    md.append("")
    md.append("## 6) ä¸‹æœˆè¡Œå‹•æ¸…å–®ï¼ˆ30 å¤©å…§åšå¾—å®Œï¼‰")
    for a in actions:
        md.append(f"- {a}")
    md.append("")
    return "\n".join(md)


# =========================
# 8) Sidebarï¼šç¯©é¸èˆ‡æ¨¡å¼
# =========================
st.sidebar.title("ğŸ« å…¨å°æ‹›ç”Ÿ GEO/AI æˆ°æƒ…å®¤")

mode = st.sidebar.radio(
    "é¸æ“‡è¦–è§’",
    ["ğŸ“Œ ç³»ä¸»ä»»ä¸€é å¼", "ğŸ§­ å…¨æ ¡/å­¸é™¢ç¸½è¦½", "ğŸ” å–®ç³»æˆ°æƒ…å®¤ï¼ˆTop3+Promptï¼‰"],
    index=0
)

college_list = ["å…¨éƒ¨å­¸é™¢"] + sorted(df["College"].unique().tolist())
selected_college = st.sidebar.selectbox("STEP 1: é¸æ“‡å­¸é™¢", college_list)

if selected_college == "å…¨éƒ¨å­¸é™¢":
    dept_options = sorted(df["Department"].unique().tolist())
else:
    dept_options = sorted(df[df["College"] == selected_college]["Department"].unique().tolist())

selected_dept = st.sidebar.selectbox("STEP 2: é¸æ“‡ç§‘ç³»", dept_options)

kw_types = ["å…¨éƒ¨æ„åœ–"] + sorted(df["Keyword_Type"].unique().tolist())
selected_kw_type = st.sidebar.selectbox("STEP 3: ç¯©é¸æœå°‹æ„åœ–", kw_types)

source_list = ["å…¨éƒ¨ä¾†æº"] + sorted(df["Keyword_Source"].unique().tolist())
selected_source = st.sidebar.selectbox("STEP 4: ç¯©é¸ Keyword ä¾†æº", source_list)

min_ai = st.sidebar.slider("AI_Potential æœ€ä½é–€æª»", 0, 100, 0, 5)
min_opp_max = int(max(1, df["Opportunity_Score"].max()))
min_opp = st.sidebar.slider("Opportunity_Score æœ€ä½é–€æª»", 0, min_opp_max, 0, 10)

st.sidebar.divider()
st.sidebar.caption("âœ… æƒ³çœ‹æœ€åƒçœŸäººè¼¸å…¥ï¼šä¾†æºé¸ Autocompleteã€‚")

if funnel_df is None:
    st.sidebar.caption("ï¼ˆå¯é¸ï¼‰æ”¾å…¥ funnel_data.csv å¯é¡¯ç¤ºæ¼æ–—è½‰æ›ã€‚")
if gsc_df is None:
    st.sidebar.caption("ï¼ˆå¯é¸ï¼‰æ”¾å…¥ gsc_queries.csv å¯é¡¯ç¤º Search Console çœŸå¯¦ queryã€‚")


# å¥—ç”¨ç¯©é¸
target_df = df.copy()
if selected_college != "å…¨éƒ¨å­¸é™¢":
    target_df = target_df[target_df["College"] == selected_college]

if selected_kw_type != "å…¨éƒ¨æ„åœ–":
    target_df = target_df[target_df["Keyword_Type"] == selected_kw_type]

if selected_source != "å…¨éƒ¨ä¾†æº":
    target_df = target_df[target_df["Keyword_Source"] == selected_source]

target_df = target_df[target_df["AI_Potential"] >= min_ai]
target_df = target_df[target_df["Opportunity_Score"] >= min_opp]


# =========================
# 9) å…¨æ ¡/å­¸é™¢ç¸½è¦½
# =========================
def overview_page(scope_df: pd.DataFrame, title_prefix: str):
    st.title(f"ğŸ§­ {title_prefix}ï½œç¸½è¦½ï¼ˆGEO/AI æŒ‡æ¨™ + ä¾†æºçµæ§‹ï¼‰")

    vcol = prefer_volume_col(scope_df)
    vlabel = "Trends ç›¸å°è²é‡" if vcol == "Trends_Score" else "è²é‡æŒ‡æ¨™"

    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: st.metric("é—œéµå­—ç­†æ•¸", int(len(scope_df)))
    with c2: st.metric("å¹³å‡ Opportunity", round(scope_df["Opportunity_Score"].mean(), 1) if len(scope_df) else 0)
    with c3: st.metric("å¹³å‡ AI", round(scope_df["AI_Potential"].mean(), 1) if len(scope_df) else 0)
    with c4: st.metric("å¹³å‡ Citable", round(scope_df["Citable_Score"].mean(), 1) if len(scope_df) else 0)
    with c5: st.metric(f"å¹³å‡ {vlabel}", round(scope_df[vcol].mean(), 2) if len(scope_df) else 0)

    st.divider()

    left, right = st.columns([2, 1])
    with left:
        dept_rank = (
            scope_df.groupby("Department", as_index=False)["Opportunity_Score"]
            .mean()
            .sort_values("Opportunity_Score", ascending=False)
        )
        fig = px.bar(dept_rank, x="Department", y="Opportunity_Score", color="Department",
                     title="å„ç³» GEO æ©Ÿæœƒå€¼æ’è¡Œï¼ˆå¹³å‡ Opportunityï¼‰")
        st.plotly_chart(fig, use_container_width=True)

    with right:
        fig2 = px.pie(scope_df, names="Keyword_Type", title="æœå°‹æ„åœ–åˆ†ä½ˆ")
        st.plotly_chart(fig2, use_container_width=True)

    st.divider()

    colA, colB = st.columns(2)
    with colA:
        src_rank = (
            scope_df.groupby("Keyword_Source", as_index=False)
            .size()
            .rename(columns={"size": "Count"})
            .sort_values("Count", ascending=False)
        )
        fig3 = px.bar(src_rank, x="Keyword_Source", y="Count", color="Keyword_Source",
                      title="Keyword ä¾†æºåˆ†ä½ˆï¼ˆè¶Šå¤š autocomplete è¶ŠåƒçœŸäººï¼‰")
        st.plotly_chart(fig3, use_container_width=True)

    with colB:
        vol_rank = (
            scope_df.groupby("Department", as_index=False)[vcol]
            .mean()
            .sort_values(vcol, ascending=False)
        )
        fig4 = px.bar(vol_rank, x="Department", y=vcol, color="Department",
                      title=f"å„ç³» {vlabel}ï¼ˆå¹³å‡ï¼‰")
        st.plotly_chart(fig4, use_container_width=True)

    st.divider()
    st.subheader("ğŸ“‹ é—œéµå­—ç¸½è¡¨ï¼ˆå«ä¾†æºèˆ‡è­‰æ“šï¼‰")

    show_cols = [
        "College","Department","Keyword","Keyword_Source","Seed_Term",
        "Keyword_Type","Opportunity_Score","AI_Potential","Citable_Score",
        "Authority_Count","Forum_Count", vcol, "Trends_Fetched", "Rank1_Title"
    ]
    show_cols = [c for c in show_cols if c in scope_df.columns]

    st.dataframe(
        scope_df[show_cols].sort_values(["Opportunity_Score","AI_Potential"], ascending=False),
        use_container_width=True,
        height=640
    )


# =========================
# 10) ç³»ä¸»ä»»ä¸€é å¼
# =========================
def onepager_page(scope_df: pd.DataFrame, dept_name: str):
    dept_df = scope_df[scope_df["Department"] == dept_name].copy()
    if dept_df.empty:
        st.warning("é€™å€‹ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰è³‡æ–™ï¼ˆå¯æŠŠé–€æª»èª¿ä½æˆ–å–æ¶ˆä¾†æº/æ„åœ–ç¯©é¸ï¼‰ã€‚")
        st.stop()

    dept_df = dept_df.sort_values(["Opportunity_Score","AI_Potential"], ascending=False)
    vcol = prefer_volume_col(dept_df)
    vlabel = "Trends ç›¸å°è²é‡" if vcol == "Trends_Score" else "è²é‡æŒ‡æ¨™"

    st.title(f"ğŸ“Œ {dept_name}ï½œç³»ä¸»ä»»ä¸€é å¼ï¼ˆç”¨ã€çœŸå¯¦æ±ºç­–ä¾æ“šã€èªªæœï¼‰")

    # å¿«ç…§ KPIï¼ˆæ²’æœ‰æ¼æ–—ä¹Ÿèƒ½å…ˆè·‘ï¼‰
    snap = {
        "n": int(len(dept_df)),
        "opp": round(dept_df["Opportunity_Score"].mean(), 1),
        "ai": round(dept_df["AI_Potential"].mean(), 1),
        "citable": round(dept_df["Citable_Score"].mean(), 1),
        "vol": round(dept_df[vcol].mean(), 2),
        "vol_label": vlabel
    }

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("é—œéµå­—ç­†æ•¸", snap["n"])
    c2.metric("å¹³å‡ Opportunity", snap["opp"])
    c3.metric("å¹³å‡ AI", snap["ai"])
    c4.metric("å¹³å‡ Citable", snap["citable"])
    c5.metric(f"å¹³å‡ {vlabel}", snap["vol"])

    # å¯é¸ï¼šæ¼æ–—è³‡æ–™
    if funnel_df is not None and "Department" in funnel_df.columns:
        fd = funnel_df[funnel_df["Department"] == dept_name]
        if not fd.empty:
            st.divider()
            st.subheader("ğŸ§ª ç”³è«‹æ¼æ–—ï¼ˆå¯é¸ï¼šä¾†è‡ª funnel_data.csvï¼‰")
            row = fd.iloc[0].to_dict()
            steps = ["Exposure", "Click", "Lead", "Visit", "Enroll"]
            cols = st.columns(len(steps))
            vals = []
            for i, s in enumerate(steps):
                v = row.get(s, None)
                vals.append(v)
                cols[i].metric(s, int(v) if pd.notna(v) else 0)
            # è½‰æ›ç‡
            try:
                exp = float(row.get("Exposure", 0) or 0)
                lead = float(row.get("Lead", 0) or 0)
                visit = float(row.get("Visit", 0) or 0)
                enroll = float(row.get("Enroll", 0) or 0)
                st.caption(f"ç²—è½‰æ›ï¼šæ›å…‰â†’ç•™è³‡ {lead/max(1,exp):.1%}ï½œç•™è³‡â†’åˆ°è¨ª {visit/max(1,lead):.1%}ï½œåˆ°è¨ªâ†’å ±åˆ° {enroll/max(1,visit):.1%}")
            except Exception:
                pass

    # å¯é¸ï¼šGSC çœŸå¯¦ query
    if gsc_df is not None and "Department" in gsc_df.columns:
        gd = gsc_df[gsc_df["Department"] == dept_name]
        if not gd.empty:
            st.divider()
            st.subheader("ğŸ” Search Console çœŸå¯¦ Queryï¼ˆå¯é¸ï¼šä¾†è‡ª gsc_queries.csvï¼‰")
            show = gd.copy()
            for col in ["Impressions", "Clicks", "Position"]:
                if col in show.columns:
                    show[col] = pd.to_numeric(show[col], errors="coerce").fillna(0)
            st.dataframe(show.sort_values("Impressions", ascending=False).head(20), use_container_width=True, height=360)

    # ç«¶å“ Top5
    st.divider()
    st.subheader("ğŸ« ä¸»è¦ç«¶å“ Top5ï¼ˆå¾ Top3 SERP æ¨™é¡Œ/ç¶²åŸŸæ¨ä¼°ï¼‰")
    comp_top5 = competitor_top5_from_dept(dept_df)
    if comp_top5:
        st.dataframe(pd.DataFrame(comp_top5), use_container_width=True, height=220)
    else:
        st.info("ç›®å‰æŠ“åˆ°çš„ SERP è³‡è¨Šä¸è¶³ä»¥æ¨ä¼°ç«¶å“ï¼ˆå¯é™ä½ç¯©é¸é–€æª»æˆ–è®“ powergeo å¤šæŠ“ä¸€äº› keywordï¼‰ã€‚")

    # æ±ºç­–å•é¡Œ Top10 + åˆ†é¡
    st.divider()
    st.subheader("ğŸ§  å­¸ç”Ÿæ±ºç­–ä¾æ“šï¼šä»–å€‘å…¶å¯¦åœ¨å•ä»€éº¼ï¼Ÿ")
    top10_q, cat_rows = decision_questions_top10(dept_df)

    left, right = st.columns([1.2, 1])
    with left:
        st.markdown("**Top10 åŸå¥å•é¡Œï¼ˆæœ€åƒçœŸäººï¼‰**")
        if top10_q:
            st.dataframe(pd.DataFrame(top10_q), use_container_width=True, height=320)
        else:
            st.caption("ï¼ˆæ²’æœ‰æ˜é¡¯å•å¥ï¼Œå»ºè­°ä¾†æºç¯© Autocomplete æˆ–æŠŠé–€æª»èª¿ä½ï¼‰")

    with right:
        st.markdown("**åˆ†é¡å æ¯”ï¼ˆç³»ä¸»ä»»çœ‹é€™å€‹å°±æ‡‚å­¸ç”Ÿåœ¨æ„ä»€éº¼ï¼‰**")
        if cat_rows:
            fig = px.bar(pd.DataFrame(cat_rows), x="Category", y="Share", color="Category", title="æ±ºç­–å•é¡Œå æ¯”ï¼ˆ%ï¼‰")
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(pd.DataFrame(cat_rows), use_container_width=True, height=220)
        else:
            st.caption("ï¼ˆå°šç„¡åˆ†é¡çµæœï¼‰")

    # å…§å®¹ç¼ºå£
    st.divider()
    st.subheader("ğŸ§© å…§å®¹ç¼ºå£ï¼ˆç¾åœ¨ç¶²è·¯ä¸Šå¸¸ç¼ºã€ä½†å­¸ç”Ÿå¾ˆåœ¨æ„ï¼‰")
    gaps = content_gap_suggestions(dept_df)
    for g in gaps:
        st.write(f"- {g}")

    # ä¸‹æœˆè¡Œå‹•æ¸…å–®
    st.divider()
    st.subheader("âœ… ä¸‹æœˆè¡Œå‹•æ¸…å–®ï¼ˆ30 å¤©å…§åšå¾—å®Œï¼‰")
    actions = next_30_days_action_plan(dept_df, top10_q, comp_top5)
    for a in actions:
        st.write(f"- {a}")

    # ä¸€éµåŒ¯å‡ºçµ¦ç³»ä¸»ä»»ï¼ˆMarkdownï¼‰
    st.divider()
    st.subheader("ğŸ“¤ åŒ¯å‡ºï¼ˆçµ¦ç³»ä¸»ä»»/ç°¡å ±ç”¨ï¼‰")
    md = build_onepager_markdown(dept_name, snap, comp_top5, cat_rows, top10_q, gaps, actions)
    st.download_button(
        label="ä¸‹è¼‰ç³»ä¸»ä»»ä¸€é å¼ï¼ˆMarkdownï¼‰",
        data=md.encode("utf-8"),
        file_name=f"{dept_name}_ç³»ä¸»ä»»ä¸€é å¼.md",
        mime="text/markdown"
    )
    with st.expander("é è¦½ Markdown", expanded=False):
        st.code(md, language="markdown")


# =========================
# 11) å–®ç³»æˆ°æƒ…å®¤ï¼ˆTop3 + Evidence + Prompt æ³¨å…¥ + å¯é¸æ·±åº¦è§£æï¼‰
# =========================
def warroom_page(scope_df: pd.DataFrame, dept_name: str):
    dept_df = scope_df[scope_df["Department"] == dept_name].copy()
    if dept_df.empty:
        st.warning("é€™å€‹ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰è³‡æ–™ï¼ˆå¯æŠŠé–€æª»èª¿ä½æˆ–å–æ¶ˆä¾†æº/æ„åœ–ç¯©é¸ï¼‰ã€‚")
        st.stop()

    dept_df = dept_df.sort_values(["Opportunity_Score","AI_Potential"], ascending=False)
    vcol = prefer_volume_col(dept_df)
    vlabel = "Trends ç›¸å°è²é‡" if vcol == "Trends_Score" else "è²é‡æŒ‡æ¨™"

    st.title(f"ğŸ” {dept_name}ï½œå–®ç³»æˆ°æƒ…å®¤ï¼ˆTop3 + Promptï¼‰")

    # é¸ keyword
    dept_df["Display_Label"] = (
        dept_df["Keyword"] + " ã€”" +
        dept_df["Keyword_Type"] + " / " +
        dept_df["Keyword_Source"].apply(source_tag) + "ã€•"
    )
    target_label = st.selectbox("é¸æ“‡é—œéµå­—", dept_df["Display_Label"].unique())
    target_row = dept_df[dept_df["Display_Label"] == target_label].iloc[0]

    kw = safe_str(target_row["Keyword"])
    kw_type = safe_str(target_row["Keyword_Type"])
    strategy = safe_str(target_row["Strategy_Tag"])
    src = safe_str(target_row["Keyword_Source"])
    seed = safe_str(target_row["Seed_Term"])
    evidence = safe_str(target_row.get("Evidence", "ç„¡"))

    st.caption(f"ä¾†æºï¼š{source_tag(src)}ï½œSeedï¼š{seed}ï½œæ„åœ–ï¼š{kw_type}")

    if evidence != "ç„¡" and evidence.strip():
        with st.expander("ğŸ” Evidenceï¼ˆç‚ºä»€éº¼èªªé€™ä¸æ˜¯ä½ ç·¨çš„ï¼‰", expanded=False):
            st.code(evidence[:800])

    # æ·±åº¦è§£æï¼ˆå¯é¸ï¼‰
    deep_on = False
    run_deep = False
    if HAS_REQUESTS:
        deep_on = st.checkbox("å•Ÿç”¨æ·±åº¦è§£æï¼šæŠ“ Top3 ç¶²é ï¼ˆç¬¬ä¸€æ¬¡æ…¢ã€æœ‰å¿«å–ï¼‰", value=False)
        run_deep = st.button("é–‹å§‹æ·±åº¦è§£æ Top3")
    else:
        st.info("è‹¥è¦æ·±åº¦è§£æ Top3 ç¶²é ï¼šè«‹ pip install requests beautifulsoup4")

    st.divider()

    # å·¦ï¼šæŒ‡æ¨™
    col_l, col_r = st.columns([1, 2])
    with col_l:
        st.metric("Opportunity", round(float(target_row["Opportunity_Score"]), 1))
        st.metric("AI_Potential", int(target_row["AI_Potential"]))
        st.metric("Citable", round(float(target_row["Citable_Score"]), 1))
        st.metric(vlabel, round(float(target_row.get(vcol, 0)), 2))
        st.metric("Authority", int(target_row["Authority_Count"]))
        st.metric("Forum", int(target_row["Forum_Count"]))

        st.caption("çµæ§‹åŒ–ç·šç´¢ï¼ˆè¶Šå¤šè¶Šå®¹æ˜“è¢« AI æ‘˜éŒ„ï¼‰")
        s_cols = st.columns(4)
        s_cols[0].metric("FAQ", int(target_row["Has_FAQ"]))
        s_cols[1].metric("Table", int(target_row["Has_Table"]))
        s_cols[2].metric("List", int(target_row["Has_List"]))
        s_cols[3].metric("H2/H3", int(target_row["Has_Headings"]))

        st.info(f"ç­–ç•¥ï¼š{strategy}")

    # å³ï¼šTop3 + æ·±åº¦è§£ææ‘˜è¦
    competitor_info_text = ""
    deep_briefs = []
    gap_pool_h2 = []
    agg_number_clues = {"salary": [], "score": [], "credits": [], "passrate": []}

    with col_r:
        st.markdown(f"### ğŸ‘€ ã€Œ{kw}ã€Top 3 æœå°‹çµæœ")
        for i in range(1, 4):
            title = safe_str(target_row.get(f"Rank{i}_Title", "ç„¡"))
            link = safe_str(target_row.get(f"Rank{i}_Link", "#"))
            snippet = safe_str(target_row.get(f"Rank{i}_Snippet", ""))

            if title == "ç„¡":
                continue

            competitor_info_text += f"{i}. æ¨™é¡Œï¼š{title}\n   æ‘˜è¦ï¼š{clip_text(snippet, 140)}\n"

            with st.container(border=True):
                st.markdown(f"**#{i} [{title}]({link})**")
                if snippet.strip():
                    st.caption(clip_text(snippet, 260))

            if deep_on and run_deep and link not in ["#", "ç„¡", ""]:
                info = parse_competitor_page(link)
                if info.get("ok") == 1:
                    deep_briefs.append((i, info))
                    gap_pool_h2.extend(info.get("h2", [])[:15])

                    nc = info.get("number_clues", {}) or {}
                    for k in agg_number_clues.keys():
                        agg_number_clues[k].extend(nc.get(k, []))

    # Content Gap + ç†æ€§å¼•ç”¨æ®µè½ï¼ˆä¾†è‡ªæ·±åº¦è§£æï¼‰
    gap_suggestions = []
    if deep_briefs:
        top1_h2 = set(deep_briefs[0][1].get("h2", []))
        freq = Counter([h for h in gap_pool_h2 if 4 <= len(h) <= 24])
        for h, _ in freq.most_common(12):
            if h not in top1_h2:
                gap_suggestions.append(h)
        gap_suggestions = gap_suggestions[:8]

    for k in agg_number_clues:
        agg_number_clues[k] = _dedup_keep_order(agg_number_clues[k], max_n=12)

    human = humanize_number_output(agg_number_clues)
    rational_paras = build_rational_citation_paragraphs(human)

    if deep_on and run_deep:
        st.divider()
        st.subheader("ğŸ“Œ æ·±åº¦è§£æï¼šæ•¸å­—ç·šç´¢ï¼ˆæ›´åƒäººé¡çš„ç†æ€§å¯«æ³•ï¼‰")
        with st.container(border=True):
            st.markdown(rational_paras)

        if gap_suggestions:
            st.subheader("ğŸ§© Content Gapï¼ˆTop1 æ²’è¬›ã€ä½†å…¶ä»–äººå¸¸æï¼‰")
            for g in gap_suggestions:
                st.write(f"- {g}")

    # Prompt ç”Ÿæˆï¼ˆæ³¨å…¥ä¾†æºè­‰æ“š + ç†æ€§å¼•ç”¨æ®µè½ï¼‰
    st.divider()
    st.subheader("âœï¸ AI æ™ºèƒ½æ–‡æ¡ˆç”Ÿæˆå™¨ï¼ˆæ³¨å…¥ä¾†æºè­‰æ“š + ç†æ€§å¼•ç”¨æ®µè½ï¼‰")

    template_type = st.radio(
        "æ–‡ç« æ‰“æ³•",
        ["âš”ï¸ ç†æ€§ç«¶çˆ­å‹ï¼ˆå°ç…§è¡¨ + ç¼ºå£è£œé½Šï¼‰", "ğŸ† ç†æ€§æ¬Šå¨å‹ï¼ˆåˆ¶åº¦/å¼•ç”¨å„ªå…ˆï¼‰", "ğŸ¤– AI å‹å–„å‹ï¼ˆè¡¨æ ¼+FAQ+å¯æ‘˜éŒ„ï¼‰"],
        horizontal=True
    )

    if "ç«¶çˆ­å‹" in template_type:
        base_instruction = "ä¸»å¼µè¦å¯æª¢æ ¸ï¼šæ¯”è¼ƒç”¨è¡¨æ ¼ï¼Œçµè«–ç”¨è­‰æ“šã€‚"
        structure_req = (
            "1) TL;DRï¼ˆ4â€“6 è¡Œï¼‰\n"
            "2) å°ç…§è¡¨ï¼šæœ¬æ ¡ vs Top1ï¼ˆèª²ç¨‹/å¯¦ç¿’/è­‰ç…§/å‡ºè·¯/è³‡æºï¼‰\n"
            "3) Content Gap ä¸€æ¬¡è£œé½Šï¼ˆè‡³å°‘ 6 é»ï¼‰\n"
            "4) FAQ 8â€“12 é¡Œï¼ˆçŸ­ã€ç›´æ¥ã€å¯æ‘˜éŒ„ï¼‰\n"
        )
    elif "æ¬Šå¨å‹" in template_type:
        base_instruction = "ä»¥åˆ¶åº¦èˆ‡å¯æŸ¥è³‡æ–™å»ºç«‹ä¿¡ä»»ï¼šå…¥å­¸ã€èª²ç¨‹ã€å¯¦ç¿’ã€è€ƒç…§ã€å°±æ¥­ã€‚"
        structure_req = (
            "1) å…¥å­¸ç®¡é“èˆ‡é–€æª»ï¼ˆè¿‘ 2â€“3 å¹´å€é–“ï¼‹ä¾†æºï¼‰\n"
            "2) å­¸åˆ†çµæ§‹èˆ‡èª²ç¨‹åœ°åœ–ï¼ˆè¡¨æ ¼ï¼‰\n"
            "3) å¯¦ç¿’èˆ‡è€ƒç…§ï¼ˆæµç¨‹åŒ–ï¼‰\n"
            "4) å‡ºè·¯èˆ‡è–ªè³‡ï¼ˆå€é–“ + å¹´è³‡/è·å‹™ï¼‰\n"
            "5) FAQ è‡³å°‘ 6 é¡Œ\n"
        )
    else:
        base_instruction = "å¯«æˆ AI æœ€å¥½æ‘˜è¦çš„æ ¼å¼ï¼šçŸ­æ®µè½ã€è¡¨æ ¼ã€æ¢åˆ—ã€FAQï¼Œä¸¦æ¨™ç¤ºå¼•ç”¨ä¾†æºé¡å‹ã€‚"
        structure_req = (
            "1) TL;DRï¼ˆ5 è¡Œï¼‰\n"
            "2) æ ¸å¿ƒè¡¨æ ¼ï¼ˆè‡³å°‘ 1 å¼µï¼‰\n"
            "3) æ­¥é©Ÿæ¸…å–®ï¼ˆé¢è©¦/é¸èª²/è€ƒç…§ä»»ä¸€ï¼‰\n"
            "4) FAQ è‡³å°‘ 10 é¡Œ\n"
        )

    deep_text_for_prompt = ""
    if deep_briefs:
        deep_text_for_prompt += "\n# ğŸ§  ç«¶å“æ·±åº¦æ‘˜è¦ï¼ˆä½ å·²è®€é Top3 çš„çµæ§‹ï¼‰\n"
        for idx, info in deep_briefs:
            deep_text_for_prompt += (
                f"- #{idx} {domain_of(info['url'])}\n"
                f"  - H1: {safe_str(info.get('h1','ç„¡'))}\n"
                f"  - H2: {', '.join(info.get('h2', [])[:10])}\n"
                f"  - Struct: FAQ={info.get('has_faq',0)}, Table={info.get('has_table',0)}, List={info.get('has_list',0)}\n"
            )

    gap_text = ""
    if gap_suggestions:
        gap_text = "\n# ğŸ§© å»ºè­°è£œå¼·å…§å®¹ç¼ºå£\n" + "\n".join([f"- {g}" for g in gap_suggestions]) + "\n"

    cite_block = "\n# ğŸ“ å»ºè­°å¼•ç”¨æ®µè½ï¼ˆç†æ€§ç‰ˆï¼Œå¯ç›´æ¥è²¼ï¼‰\n" + rational_paras + "\n"

    final_prompt = f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä½åç†æ€§ã€é‡è¦–å¯æŸ¥è³‡æ–™èˆ‡çµæ§‹åŒ–å‘ˆç¾çš„ SEO + GEO å…§å®¹ç­–ç•¥é¡§å•ã€‚

# ä»»å‹™
ç‚ºã€Œ{dept_name}ã€å¯«ä¸€ç¯‡è¦è¡æ’åã€ä¹Ÿè¦å®¹æ˜“è¢« AI æ‘˜éŒ„/å¼•ç”¨çš„æ–‡ç« ã€‚
ç›®æ¨™é—œéµå­—ï¼š**{kw}**

# é—œéµå­—ä¾†æºï¼ˆæé«˜å¯ä¿¡åº¦ï¼Œè«‹åœ¨æ–‡ä¸­äº¤ä»£ï¼‰
- Keyword_Sourceï¼š{src}
- Seed_Termï¼š{seed}
- Evidenceï¼š{evidence}

# ç›®å‰ Top3 åœ¨è¬›ä»€éº¼ï¼ˆæ‘˜è¦ï¼‰
{competitor_info_text}
{deep_text_for_prompt}
{gap_text}
{cite_block}

# å¯«ä½œç­–ç•¥
{base_instruction}

# çµæ§‹ï¼ˆç…§åšï¼‰
{structure_req}
5) æ–‡æœ«åŠ  3 é¡Œã€å¤§å®¶æœ€å¸¸å•ã€Q&A + CTAï¼ˆç³»ç¶²/åƒè¨ª/è«®è©¢ï¼‰

# Constraints
- ç”¨ Markdownï¼ˆH2/H3 æ¸…æ¥šï¼Œè¡¨æ ¼è¦èƒ½å¿«é€Ÿæƒè®€ï¼‰
- èªæ°£åç†æ€§ï¼šé¿å…å£è™Ÿå¼å½¢å®¹è©ï¼Œä¸»å¼µè¦èƒ½è¢«æª¢æ ¸
- æ¶‰åŠæ•¸æ“šï¼ˆè–ªè³‡/åˆ†æ•¸/å­¸åˆ†/åŠæ ¼ç‡ï¼‰ç”¨ã€å€é–“ã€ï¼Œä¸¦äº¤ä»£ã€å¹´ä»½/ä¾†æºé¡å‹ã€
"""
    st.text_area("ğŸ“‹ è¤‡è£½ Promptï¼š", final_prompt, height=680)
    st.success("âœ… Prompt å·²æ³¨å…¥ä¾†æºè­‰æ“š + ç†æ€§å¼•ç”¨æ®µè½ï¼Œæ–‡ç« æœƒæ›´åƒçœŸçš„åšéè³‡æ–™ã€‚")


# =========================
# 12) è·¯ç”±
# =========================
if mode.startswith("ğŸ§­"):
    title_prefix = "å…¨æ ¡" if selected_college == "å…¨éƒ¨å­¸é™¢" else selected_college
    overview_page(target_df, title_prefix)
elif mode.startswith("ğŸ“Œ"):
    onepager_page(target_df, selected_dept)
else:
    warroom_page(target_df, selected_dept)
